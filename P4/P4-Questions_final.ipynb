{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"IvcOz1eTrxRx"},"source":["# **Deep Generative Models: Variational Autoencoders and Generative Adversarial Networks**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EvWs2XIYNfhg"},"source":["# **CK Dataset**\n","In the following exercices, you will work with images extracted from the CK dataset: http://www.jeffcohn.net/wp-content/uploads/2020/02/Cohn-Kanade_Database.pdf.pdf\n","\n","It contains gray-scale images of human faces.\n","\n","The dataset is provided in the folder Data/faces/ in .mat format.\n","In the following we provide a Dataset class in pytorch to load images from this database."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The aim of this is lab is to get into Deep Generative Models, especially  with Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN). We will create these models and train them with 50000 extracted images from the Cohn-Kanade Database. These images have a size of 32x32 pixels and are gray-scale. They consist of human faces with different emotions."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"executionInfo":{"elapsed":43049,"status":"ok","timestamp":1653800688560,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"5kE5D0hh1Ndz","outputId":"9483add6-4ad6-45c9-f5df-a2326394b6d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Path is not defined, initialized to c:/Users/santo/Documents/GitHub/DeepLearning/P4\n","torch.Size([256, 1, 64, 64])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["## Create a Custom Dataset for CK database\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import os\n","import time\n","\n","# Mount Google Drive\n","\n","try:\n","    print(\"Currently on path\", base_path)\n","except NameError:\n","    base_path = os.getcwd().replace('\\\\', '/')\n","    print(\"Path is not defined, initialized to\", base_path)\n","\n","data_path = base_path+'/Data/'\n","results_path = base_path+'/Results/'\n","\n","# All the data will be loaded from the provided file in Data/mnist.t\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import scipy.io as sio\n","from torchvision.utils import make_grid\n","\n","\n","\n","#Making native class loader\n","class FacesDB(torch.utils.data.Dataset):\n","    # Initialization method for the dataset\n","    def __init__(self,dataDir = data_path+'/faces/face_ims_64x64.mat', transform = None):\n","        mat_loaded = sio.loadmat(dataDir)\n","        self.data = mat_loaded['X']\n","        self.transform = transform\n","\n","    # What to do to load a single item in the dataset ( read image and label)    \n","    def __getitem__(self, index):\n","        data = self.data[:,:,0,index]   \n","        data = Image.fromarray(data,mode='L')\n","        # Apply a trasnformaiton to the image if it is indicated in the initalizer\n","        if self.transform is not None : \n","            data = self.transform(data)\n","        \n","        # return the image and the label\n","        return data\n","\n","    # Return the number of images\n","    def __len__(self):\n","        return self.data.shape[3]\n","\n","import torchvision.transforms as transforms\n","\n","tr = transforms.Compose([\n","        transforms.ToTensor(), \n","        ])\n","faces_db = FacesDB(data_path+'/faces/face_ims_64x64.mat',tr)\n","train_loader = torch.utils.data.DataLoader(dataset=faces_db,\n","                                           batch_size=256, \n","                                           shuffle=True)\n","\n","# Mini-batch images\n","images = next(iter(train_loader))\n","print(images.shape)\n","image = images[0,:,:,:].repeat(3,1,1)\n","plt.imshow(image.permute(1,2,0).squeeze().numpy())\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["50000\n"]}],"source":["#amount of images\n","print(len(faces_db))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CVA6m-IgNace"},"source":["# Ex. 1"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1653800688561,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"y1yTECVjDVmf","outputId":"5285d823-5808-45c8-bbd9-c18510b02767"},"outputs":[{"data":{"text/plain":["'\\n1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\\n2. For every two epochs during training:\\n  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\\n  2.2. Generate and show a set of images from random noise z. \\n  2.3. Visualize a set of generated images by interpolating over the latent space z.\\n  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\\n'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# Convolution + BatchNormnalization + ReLU block for the encoder\n","class ConvBNReLU(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(ConvBNReLU, self).__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.AvgPool2d(2,2)\n","\n","  def forward(self,x):\n","    if(self.pool):\n","      out = self.pool(x)\n","    else:\n","      out = x\n","    out = self.relu(self.bn(self.conv(out)))   \n","    return out\n","\n","#  BatchNormnalization + ReLU block + Convolution for the decoder\n","class BNReLUConv(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(BNReLUConv, self).__init__()\n","    self.bn = nn.BatchNorm2d(in_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.UpsamplingNearest2d(scale_factor=2)\n","\n","  def forward(self,x):\n","    out = self.relu(self.bn(x))\n","    if(self.pool):\n","      out = self.pool(out)\n","    out = self.conv(out)\n","    return out\n","\n","# Encoder definition with 3 COnv-BN-ReLU blocks and fully-connected layer\n","class Encoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Encoder, self).__init__()\n","    self.layer1 = ConvBNReLU(1,base_channels,pooling=False)\n","    self.layer2 = ConvBNReLU(base_channels,base_channels*2,pooling=True)\n","    self.layer3 = ConvBNReLU(base_channels*2,base_channels*4,pooling=True)\n","    self.fc = nn.Linear(16*16*base_channels*4,out_features)\n","  \n","  def forward(self,x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    return self.fc(out.view(x.shape[0],-1))\n","    \n","# Decoder definition with a fully-connected layer and 3 BN-ReLU-COnv blocks and \n","class Decoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Decoder, self).__init__()\n","    self.base_channels = base_channels\n","    self.fc = nn.Linear(out_features,16*16*base_channels*4)\n","    self.layer3 = BNReLUConv(base_channels*4,base_channels*2,pooling=True)\n","    self.layer2 = BNReLUConv(base_channels*2,base_channels,pooling=True)\n","    self.layer1 = BNReLUConv(base_channels,1,pooling=False)\n","  \n","  def forward(self,x):\n","    out = self.fc(x)\n","    out = out.view(x.shape[0],self.base_channels*4,16,16)\n","    out = self.layer3(out)\n","    out = self.layer2(out)\n","    out = self.layer1(out)\n","    return torch.sigmoid(out)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MNISTK VAE Definition\n","VAE(\n","  (encoder): Encoder(\n","    (layer1): ConvBNReLU(\n","      (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (layer2): ConvBNReLU(\n","      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (layer3): ConvBNReLU(\n","      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (fc): Linear(in_features=16384, out_features=64, bias=True)\n","  )\n","  (decoder): Decoder(\n","    (fc): Linear(in_features=32, out_features=16384, bias=True)\n","    (layer3): BNReLUConv(\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (pool): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n","    )\n","    (layer2): BNReLUConv(\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (pool): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n","    )\n","    (layer1): BNReLUConv(\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n",")\n"]}],"source":["class VAE(nn.Module):\n","  def __init__(self, out_features=64,base_channels=16):\n","    super(VAE, self).__init__()\n","    # Initialize the encoder and decoder using a dimensionality out_features for the vector z\n","    self.out_features = out_features\n","    self.encoder = Encoder(out_features*2,base_channels)\n","    self.decoder = Decoder(out_features,base_channels)\n","\n","  # function to obtain the mu and sigma of z for a samples x\n","  def encode(self,x):\n","    aux = self.encoder(x)\n","    # get z mean\n","    z_mean = aux[:,0:self.out_features]\n","    # get z variance\n","    z_log_var = aux[:,self.out_features::]\n","    return z_mean, z_log_var\n","\n","  # function to generate a random sample z given mu and sigma\n","  def sample_z(self,z_mean,z_log_var):\n","    z_std = z_log_var.mul(0.5).exp() \n","    samples_unit_normal = torch.randn_like(z_mean)\n","    samples_z = samples_unit_normal*z_std + z_mean\n","    return samples_z\n","  \n","  def forward(self,x):\n","    z_mean, z_log_var = self.encode(x)\n","    samples_z = self.sample_z(z_mean,z_log_var)\n","    x_rec = self.decoder(samples_z)\n","    return x_rec, z_mean, z_log_var\n","\n","# Print summary ofCmode\n","print('MNISTK VAE Definition')\n","vae = VAE(32)\n","print(vae)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["## Kullback-Leibler regularization computation\n","def kl_divergence(z_mean,z_log_var):\n","  kl_loss = 0.5 * torch.sum(  (torch.exp(z_log_var) + z_mean**2 - 1.0 - z_log_var),axis=1)\n","  return kl_loss.mean()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TPhaB5uN5nub"},"source":["## Sol. 1"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nFor every two epochs during training:\\n  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\\n  2.2. Generate and show a set of images from random noise z. \\n  2.3. Visualize a set of generated images by interpolating over the latent space z.\\n  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\\n'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["import imageio\n","\n","def generate_images(vae, kl_weigt,n_samples=64, device='cpu',variance=1.0,mean=0.0):\n","    ### Generate random samples\n","    vae.eval()\n","    # Random vectors z~N(0,I)\n","    z = torch.randn((n_samples,vae.out_features)).to(device)\n","    if mean==\"Random\":\n","        mean = 2*torch.rand((n_samples,vae.out_features)).to(device) -1*torch.ones((n_samples,vae.out_features)).to(device)\n","    z = z*variance + mean\n","    # Genearte images with the decoder from the random vectors\n","    x_rec = vae.decoder(z)\n","    \n","\n","    # Show synthetic images\n","    plt.figure(figsize=(9,9))\n","    plt.title('Generated Images with KL weight = {}'.format(kl_weigt),fontdict={'fontsize':20,'fontweight':'bold'})\n","    image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","    plt.imshow(image_grid.permute(1,2,0).detach().numpy(),cmap='gray')\n","    plt.show()\n","\n","def interpolate_images(vae, kl_weight, n_samples=64, n_iterpolations=50, device='cpu'):\n","    ### Generate random samples\n","    ### Generate random samples\n","    vae.eval()\n","\n","    # Sample a set of pairs z_init and z_final\n","    z_init = torch.randn((n_samples,vae.out_features)).to(device)*2.0\n","    z_final = torch.randn((n_samples,vae.out_features)).to(device)*2.0\n","\n","    # Compute interpolations between z_init and z_final\n","    # and generate an image for each interpolation.\n","    interpolation_images = []\n","    for interp in range(0,n_iterpolations):\n","        interp_0_1 = float(interp) / (n_iterpolations-1)\n","        z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","        x_rec = vae.decoder(z)\n","        image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","        image_grid = image_grid.permute(1,2,0).detach().numpy()\n","        # save the generated images in a list\n","        interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","\n","    # Concatenate the inversion of the list to generate a \"loop\" animation\n","    interpolation_images += interpolation_images[::-1]\n","\n","    # Generate and visualize a give showing the interpolation results.\n","    imname = results_path+'/vae_interpolation_CKL_'+str(kl_weight)+'.gif'\n","    imageio.mimsave(imname, interpolation_images, duration=10)\n","\n","    with open(imname,'rb') as f:\n","        from IPython.display import Image, display\n","        display(Image(data=f.read(), format='png',width=512,height=512))\n","        \n","def alternate_rec_ref(rec_images, ref_images):\n","    interleaved_images = np.empty((2 * len(ref_images), 64, 64,3), dtype=np.float32)\n","\n","    for i in range(len(ref_images)):\n","        interleaved_images[2*i] = ref_images[i]\n","        interleaved_images[2*i+1] = rec_images[i]\n","\n","    # Create the grid of images assume interleaved_images are numpy arrays\n","    plt.figure(figsize=(9,9))\n","    plt.title('Reconstructed Images')\n","    image_grid = make_grid(torch.from_numpy(interleaved_images),nrow=8,padding=1)\n","    print(image_grid.shape)\n","    plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","    plt.show()\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import random\n","\n","# Train function stopping at every 2 epochs for the requested visualizations\n","def train_VAE(vae,  train_loader, optimizer, kl_weight=0.001, num_epochs=10, model_name='vae_mnist.ckpt', device='cpu'):\n","    vae.to(device)\n","    vae.train() # Set the model in train mode\n","    total_step = len(train_loader)\n","    losses_list = []\n","    rec_loss_list = []\n","    kl_loss_list = []\n","    criterion = nn.MSELoss() # Use mean-squared error to compare the original and reconstructe images\n","    samples_images = np.random.choice(79, 16, replace=False)\n","    \n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        rec_loss_avg = 0\n","        kl_loss_avg = 0\n","        nBatches = 0\n","        \n","\n","        for i, (images) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            images = images.to(device)\n","            if epoch == 0:\n","                ref_images = images\n","\n","            # Forward pass (get encoder variables and reconstructed images)\n","            x_rec, z_mean, z_log_var = vae(images)\n","            \n","            reconstruction_loss = criterion(x_rec, images) # Reconstruction loss (x,x_rec)\n","            \n","            kl_loss = kl_divergence(z_mean, z_log_var) # Compute KL divergecnes KL( N(mu_x,sigma_x) || N(0,I))\n","            \n","            # Backward and optimize reconstruction loss and kl regularization\n","            optimizer.zero_grad()\n","            loss = reconstruction_loss + kl_loss*kl_weight # we use a weight to balance the importance of the KL loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            rec_loss_avg += reconstruction_loss.cpu().item()\n","            kl_loss_avg += kl_loss.cpu().item()\n","            loss_avg = rec_loss_avg + kl_loss_avg*kl_weight\n","\n","            nBatches+=1\n","            if (i+1) % 100 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","                       \n","        print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        losses_list.append(loss_avg / nBatches)\n","        rec_loss_list.append(rec_loss_avg / nBatches)\n","        kl_loss_list.append(kl_loss_avg / nBatches)\n","\n","        if epoch % 2 == 0:\n","              \n","            rec_images,_,_ = vae(ref_images)\n","            rec_images=rec_images.cpu()\n","            ref_images_disp=ref_images.cpu()\n","            #get 16 samples from the batch\n","            rec_images = rec_images[samples_images]\n","            ref_images_disp = ref_images_disp[samples_images]\n","\n","            # create a mixed grid with the original and reconstructed images\n","            interleaved_images =  torch.empty((2 * len(ref_images_disp), 3, 64, 64), dtype=torch.float32)\n","            interleaved_images[0::2] = ref_images_disp\n","            interleaved_images[1::2] = rec_images\n","\n","            fig=plt.figure(figsize=(11,6))\n","            plt.title('Reconstruction of the images for epoch '+str(epoch+1)+' with KL weight '+str(kl_weight),fontsize=16,fontweight='bold')\n","            plt.axis('off')\n","            fig.text(0.5,0.9,' [Rec Loss: '+str(np.round(rec_loss_avg/ nBatches,4))+' KL Loss: '+str(np.round(kl_loss_avg/ nBatches,4))+']', ha='center')\n","            image_grid = make_grid(interleaved_images,nrow=8,padding=1)\n","            plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","            \n","            #save the image\n","            imname = results_path+'/vae_reconstruction_CK_epoch_'+str(epoch+1)+ '_KL_weight_'+str(kl_weight)+'.png'\n","            plt.savefig(imname, bbox_inches='tight')\n","            plt.show()\n","            \n","            \n","            print(\"-----------------------Generated images:-----------------------\\n\")\n","            generate_images(vae, kl_weight,n_samples=16, device=device)\n","           \n","            print(\"-----------------------Interpolated images:-----------------------\\n\")\n","            interpolate_images(vae,kl_weight, n_samples=16, n_iterpolations=50, device=device)\n","\n","            # z = torch.randn_like(z_mean)\n","            # x_rec = vae.decoder(z)\n","            # plt.figure(figsize=(9,9))\n","            # plt.title('Generated Images')\n","            # image_grid = make_grid(x_rec.cpu(),nrow=4,padding=1)\n","            # plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","            # plt.show()\n","\n","    \n","    # save trained model\n","    torch.save(vae.state_dict(), results_path+ '/' + model_name)\n","          \n","    return losses_list, rec_loss_list, kl_loss_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Train a VAE on the CK dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#release GPU memory\n","torch.cuda.empty_cache()\n","\n","vae = VAE(32)\n","kl_weight=0.001 \n","\n","#Initialize optimizer \n","learning_rate = .001\n","optimizer = torch.optim.Adam(vae.parameters(),lr = learning_rate, weight_decay=1e-5)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","loss_list,rec_loss_list, kl_loss_list = train_VAE(vae, train_loader, optimizer, kl_weight=kl_weight,\n","                      num_epochs=40, model_name='vae_ck_KL_weight_'+str(kl_weight)+'.cpkt', device=device)\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.plot(loss_list, label='Total loss')\n","plt.plot(rec_loss_list, label='Reconstruction loss')\n","plt.plot(kl_loss_list, label='KL loss')\n","plt.legend()\n","plt.title('Losses for VAE with KL weight '+str(kl_weight))\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","impath = results_path+'/vae_losses_KL_weight_'+str(kl_weight)+'.png'\n","plt.savefig(impath, bbox_inches='tight')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cpu'\n","kl_weight=0.00005\n","vae_kl_0001 = VAE(32)\n","#import the trained model\n","vae_kl_0001.load_state_dict(torch.load(results_path+'/vae_ck_KL_weight_'+str(kl_weight)+'.cpkt'))\n","vae_kl_0001.eval()\n","vae_kl_0001.to(device)\n","#generate images\n","generate_images(vae_kl_0001,kl_weight, n_samples=16, device=device,variance=0.05, mean=\"Random\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# To display the reconstruction process\n","\n","from IPython.display import display, clear_output\n","%matplotlib inline\n","def animation_reconstruciont(kl_weight,sleep, epochs):\n","    plt.figure(figsize=(9,9))\n","    for i in range(epochs//2):\n","        image = plt.imread(results_path+'/vae_reconstruction_CK_epoch_'+str(2*i+1)+ '_KL_weight_'+str(kl_weight)+'.png')\n","        plt.imshow(image)\n","        plt.xticks([])\n","        plt.yticks([])\n","        display(wait=True)\n","        plt.pause(sleep)\n","        plt.show()\n","        clear_output()\n","    plt.imshow(image)\n","    plt.show()\n","\n","def gif_reconstruction(kl_weight,epochs,duration):\n","    images = []\n","    for i in range(epochs//2):\n","        images.append(imageio.imread(results_path+'/vae_reconstruction_CK_epoch_'+str(2*i+1)+ '_KL_weight_'+str(kl_weight)+'.png'))\n","    \n","    path=results_path+'/vae_reconstruction_CK_KL_weight_'+str(kl_weight)+'.gif'\n","    imageio.mimsave(path, images, duration=duration)\n","    \n","    with open(path,'rb') as f:\n","        from IPython.display import Image, display\n","        display(Image(data=f.read(), format='png'))\n","\n","        \n","\n","    \n","animation_reconstruciont(kl_weight=0.00005,sleep=0.15, epochs=40)\n","gif_reconstruction(kl_weight=0.00005,epochs=40,duration=3)\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Ex. 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'\\n1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\\n2. For every two epochs during training:\\n  2.1. Generate and show a set of images from random noise z.\\n  2.2. Visualize a set of generated images by interpolating over the latent space z.\\n  2.3. Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\\nCompare the results with the ones obtained with VAEs\\n'"]},"metadata":{},"output_type":"display_data"}],"source":["'''\n","1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Generate and show a set of images from random noise z.\n","  2.2. Visualize a set of generated images by interpolating over the latent space z.\n","  2.3. Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\n","Compare the results with the ones obtained with VAEs\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Discriminator similar to VAE encoder\n","class Discriminator(nn.Module):\n","  def __init__(self, base_channels=16):\n","    super(Discriminator, self).__init__()\n","    # last fully connected layer acts as a a binary classifier\n","    self.classifier = Encoder(1, base_channels)\n","\n","  # Forward pass obtaining the discriminator probability\n","  def forward(self,x):\n","    out = self.classifier(x)\n","    # use sigmoid to get the real/fake image probability\n","    return torch.sigmoid(out)\n","\n","# Generator is defined as VAE decoder\n","class Generator(nn.Module):\n","  def __init__(self,in_features,base_channels=16):\n","    super(Generator, self).__init__()\n","    self.base_channels = base_channels\n","    self.in_features = in_features\n","    self.decoder = Decoder(in_features,base_channels)\n","\n","  # Generate an image from vector z\n","  def forward(self,z):\n","    return torch.sigmoid(self.decoder(z))\n","\n","  # Sample a set of images from random vectors z\n","  def sample(self,n_samples=256,device='cpu'):\n","    samples_unit_normal = torch.randn((n_samples,self.in_features)).to(device)\n","    return self.decoder(samples_unit_normal)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Sol. 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'\\n1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\\n2. For every two epochs during training:\\n  2.1. Generate and show a set of images from random noise z.\\n  2.2. Visualize a set of generated images by interpolating over the latent space z.\\n  2.3. Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\\nCompare the results with the ones obtained with VAEs\\n'"]},"metadata":{},"output_type":"display_data"}],"source":["'''\n","1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Generate and show a set of images from random noise z.\n","  2.2. Visualize a set of generated images by interpolating over the latent space z.\n","  2.3. Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\n","Compare the results with the ones obtained with VAEs\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_gan(generator, n_samples=16, device='cpu'):\n","    generator = Generator(32)\n","    generator.load_state_dict(torch.load(results_path+'/gan_ck.ckpt'))\n","    generator = generator.to(device)\n","\n","\n","    generated_images = generator.sample(n_samples,device=device)\n","    plt.figure(figsize=(9,9))\n","    plt.title('Generated Images')\n","    image_grid = make_grid(generated_images.cpu(),nrow=4,padding=1)\n","    plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","    plt.show()\n","\n","def interpolate_gan(generator, n_samples = 64, n_iterpolations =50, device='cpu'):\n","    ### Generate random samples\n","    generator = Generator(32)\n","    generator.load_state_dict(torch.load(results_path+'/gan_ck.ckpt'))\n","    generator = generator.to(device)\n","    generator.eval()\n","\n","\n","    z_init = torch.randn((n_samples,generator.in_features)).to(device)\n","    z_final = torch.randn((n_samples,generator.in_features)).to(device)\n","\n","    interpolation_images = []\n","    for interp in range(0,n_iterpolations):\n","        interp_0_1 = float(interp) / (n_iterpolations-1)\n","        z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","        x_rec = generator.decoder(z.to(device))\n","        image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","        image_grid = image_grid.permute(1,2,0).detach().numpy()\n","\n","        interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","    interpolation_images += interpolation_images[::-1]\n","\n","    imname = results_path+'/gan_interpolation_ck.gif'\n","    imageio.mimsave(imname, interpolation_images, duration=10)\n","\n","    with open(imname,'rb') as f:\n","        from IPython.display import Image, display\n","        display(Image(data=f.read(), format='png',width=512,height=512))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Vanilla Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# GAN Train function. We have a generator and discriminator models and their respective optimizers.\n","def train_GAN(gen, disc,  train_loader, optimizer_gen, optimizer_disc,\n","              num_epochs=10, model_name='gan_ck.ckpt', device='cpu'):\n","    gen = gen.to(device)\n","    gen.train() # Set the generator in train mode\n","    disc = disc.to(device)\n","    disc.train() # Set the discriminator in train mode\n","\n","    total_step = len(train_loader)\n","    disc_losses_list = []\n","    gen_losses_list=[]\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        disc_loss_avg = 0\n","        gen_loss_avg = 0\n","        nBatches = 0\n","        update_generator = True\n","\n","        for i, (real_images) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            real_images = real_images.to(device)\n","            n_images = real_images.shape[0]\n","\n","            # Forward pass\n","            # Generate random images with the generator\n","            fake_images = gen.sample(n_images,device=device)\n","            \n","            # Use the discriminator to obtain the probabilties for real and generate imee\n","            prob_real = disc(real_images)\n","            prob_fake = disc(fake_images)\n","            \n","            # Generator loss\n","            gen_loss = -torch.log(prob_fake).mean()\n","            # Discriminator loss\n","            disc_loss = -0.5*(torch.log(prob_real) + torch.log(1-prob_fake)).mean()\n","\n","            \n","            # We are going to update the discriminator and generator parameters alternatively at each iteration\n","\n","            if(update_generator):\n","              # Optimize generator\n","              # Backward and optimize\n","              optimizer_gen.zero_grad()\n","              gen_loss.backward() # Necessary to not erase intermediate variables needed for computing disc_loss gradient\n","              optimizer_gen.step()\n","              update_generator = False\n","            else:           \n","              # Optimize discriminator\n","              # Backward and optimize\n","              optimizer_disc.zero_grad()\n","              disc_loss.backward()\n","              optimizer_disc.step()\n","              update_generator = True\n","                \n","\n","            disc_loss_avg += disc_loss.cpu().item()\n","            gen_loss_avg += gen_loss.cpu().item()\n","\n","            nBatches+=1\n","            if (i+1) % 200 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","        disc_losses_list.append(disc_loss_avg / nBatches)\n","        gen_losses_list.append(gen_loss_avg / nBatches)\n","        if epoch % 2 == 0:\n","          torch.save(gen.state_dict(), results_path+ '/' + model_name)\n","          print(\"-----------------------Generated images:-----------------------\\n\")\n","          generate_gan(gen, n_samples=16, device=device)\n","\n","          print(\"-----------------------Interpolated images:-----------------------\\n\")\n","          interpolate_gan(gen, n_samples = 16, n_iterpolations =50, device=device)\n","        model_epoch = f'gan_ck_{epoch}.ckpt'\n","        torch.save(gen.state_dict(), results_path+ '/' + model_name)\n","        # Save model\n","        \n","    torch.save(gen.state_dict(), results_path+ '/' + model_name)\n","    \n","          \n","    return disc_losses_list, gen_losses_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Alternative Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# GAN Train function. We have a generator and discriminator models and their respective optimizers.\n","def train_GAN(gen, disc,  train_loader, optimizer_gen, optimizer_disc,\n","              num_epochs=10, model_name='gan_ck.ckpt', device='cpu'):\n","    gen = gen.to(device)\n","    gen.train() # Set the generator in train mode\n","    disc = disc.to(device)\n","    disc.train() # Set the discriminator in train mode\n","\n","    total_step = len(train_loader)\n","    disc_losses_list = []\n","    gen_losses_list=[]\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        disc_loss_avg = 0\n","        gen_loss_avg = 0\n","        nBatches_g = 0\n","        nBatches_d = 0\n","        update_generator = epoch%2\n","\n","        for i, (real_images) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            real_images = real_images.to(device)\n","            n_images = real_images.shape[0]\n","\n","            # Forward pass\n","            # Generate random images with the generator\n","            fake_images = gen.sample(n_images,device=device)\n","\n","            # Use the discriminator to obtain the probabilties for real and generate imee\n","            prob_real = disc(real_images)\n","            prob_fake = disc(fake_images)\n","\n","            # Generator loss\n","            gen_loss = -torch.log(prob_fake).mean()\n","            # Discriminator loss\n","            disc_loss = -0.5*(torch.log(prob_real) + torch.log(1-prob_fake)).mean()\n","\n","\n","            # We are going to update the discriminator and generator parameters alternatively at each iteration\n","\n","            if(update_generator):\n","              # Optimize generator\n","              # Backward and optimize\n","              optimizer_gen.zero_grad()\n","              gen_loss.backward() # Necessary to not erase intermediate variables needed for computing disc_loss gradient\n","              optimizer_gen.step()\n","              if gen_loss < 2:\n","                update_generator = False\n","              gen_loss_avg += gen_loss.cpu().item()\n","              nBatches_d+=1\n","\n","            else:\n","              # Optimize discriminator\n","              # Backward and optimize\n","              optimizer_disc.zero_grad()\n","              disc_loss.backward()\n","              optimizer_disc.step()\n","              if disc_loss < 0.5:\n","                update_generator = True\n","              disc_loss_avg += disc_loss.cpu().item()\n","              nBatches_g+=1\n","          \n","        if nBatches_d == 0:\n","            print(\"Discriminator was not updated in epoch\", epoch)\n","            disc_losses_list.append(disc_losses_list[-1])\n","        else:\n","            disc_losses_list.append(disc_loss_avg / nBatches_d)\n","        if nBatches_g == 0:\n","            print(\"Generator was not updated in epoch\", epoch)\n","            gen_losses_list.append(gen_losses_list[-1])\n","        else:\n","            gen_losses_list.append(gen_loss_avg / nBatches_g)\n","\n","        if epoch % 2 == 0:\n","          torch.save(gen.state_dict(), results_path+ '/' + model_name)\n","          print(\"-----------------------Generated images:-----------------------\\n\")\n","          generate_gan(gen, n_samples=16, device=device)\n","\n","          print(\"-----------------------Interpolated images:-----------------------\\n\")\n","          interpolate_gan(gen, n_samples = 16, n_iterpolations =50, device=device)\n","        model_epoch = f'gan_ck_{epoch}.ckpt'\n","        torch.save(gen.state_dict(), results_path+ '/' + model_name)\n","        # Save model\n","\n","    torch.save(gen.state_dict(), results_path+ '/' + model_name)\n","\n","\n","    return disc_losses_list, gen_losses_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define Geneartor and Discriminator networks\n","gan_gen = Generator(32)\n","gan_disc = Discriminator()\n","\n","#Initialize indepdent optimizer for both networks\n","learning_rate = .0005\n","optimizer_gen = torch.optim.Adam(gan_gen.parameters(),lr = learning_rate, weight_decay=1e-5)\n","optimizer_disc = torch.optim.Adam(gan_disc.parameters(),lr = learning_rate, weight_decay=1e-5)\n","\n","# Train the GAN\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","disc_loss_list,gen_loss_list = train_GAN(gan_gen,gan_disc, train_loader, optimizer_gen, optimizer_disc,\n","                      num_epochs=20, model_name='gan_ck.ckpt', device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#plot losses\n","\n","plt.figure()\n","plt.plot(disc_loss_list,label='Discriminator loss')\n","plt.plot(gen_loss_list,label='Generator loss')\n","plt.legend()\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"albertkernel","language":"python","name":"albertkernel"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
