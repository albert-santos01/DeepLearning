{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"IvcOz1eTrxRx"},"source":["# **Deep Generative Models: Variational Autoencoders and Generative Adversarial Networks**\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rpBTf227lOaG"},"source":["## Summary\n","\n","\n","\n","**Autoencoders**: Autoencoders are usefull to create \"compressed\" representations of samples (e.g. images). In particular, they are composed of two blocks: (1) The encoder and (2) The decorder. The encoder is implemented as a network computing a vector $\\mathbf{z} \\in R^D$ from a sample $\\mathbf{x}$. If $\\mathbf{x} \\in R^{W\\times H \\times 3}$ is an image, the network is tipically a CNN composed of convolutional blocks and pooling layers. On the other hand, the decoder computes the approximate inverse of the encoder, by taking as input the vector $\\mathbf{z}$ and using a CNN with upsampling layers to compute a reconstructed image $\\mathbf{x}_{\\text{rec}}$.\n","\n","The decoder/encoder parameters are tipically optimized by minimizing the reconstruction loss (mean-squared errror) between the original sample $\\mathbf{x}$ and the reconstructed one $\\mathbf{x}_{\\text{rec}}$.\n","<figure>\n","<center>\n","<img src='https://miro.medium.com/max/1400/1*bY_ShNK6lBCQ3D9LYIfwJg@2x.png' width=\"500\" height=\"300\"/>\n","<figcaption>Vanilla Autoencoder Illustration</figcaption></center>\n","</figure>\n","\n","**Variational Autoencoders**: In many situations, we are not only interested in compress the samples $\\mathbf{x}$ but we also want to be able to generate sythetic samples from the distrbution $p(\\mathbf{x})$. Variational Autoencoders are a probabilistic model extending the vanilla AEs for this purpose. \n","\n","The main idea in VAEs is to impose a prior distribution $p(\\mathbf{z})$ (tipically a isotropic gaussian $N(0,I)$) so that we can then sample from $p(\\mathbf{z})$ and use the decoder to generate synthetic samples. Given a sample $\\mathbf{x}$, the encoder of a VAE outputs two set of variables: (i) The mean $\\mu_x$ of the distribution $p(\\mathbf{z}|\\mathbf{x})$ and the logarithm of its variance $\\sigma^2_x$. \n","\n","During training, the encoder is used to obtain these variables for each $\\mathbf{x}$ and sample a variable $\\mathbf{z}$ from a gaussian distriubtion defined by $N(\\mu_x,\\sigma)$. The sampled $\\mathbf{z}$ is used as input for the decoder to obtain a reconstructed image $\\mathbf{x}_{rec}$. To generate the sample $z \\sim N(\\mu_x,\\sigma)$, we can simply compute:\n","\n","$\\mathbf{z} = \\mu_x + \\epsilon * \\sigma, \\hspace{3mm} \\text{where} \\hspace{3mm} \\epsilon \\sim N(0,I)$.\n","\n","\n","In order to impose that the distribution $p(\\mathbf{z}|\\mathbf{x})$ is similar to the prior $p(\\mathbf{z}) \\sim N(0,I)$, a regularization is imposed over $\\mu_x$ and  $\\sigma^2_x$. In particular, the Kullback Leibler between $p(\\mathbf{z}|\\mathbf{x})$ and $p(\\mathbf{z})$ is minimized during optimization as:\n","\n","$KL(p(\\mathbf{z}|\\mathbf{x}) | p(\\mathbf{z})) = 0.5*[\\sum_i \\mu^2_i + \\sigma^2_i - \\log(\\sigma_i) -1] $\n","\n","This ensures that we will generate realistic images if we sample vectors $\\mathbf{z}$ from the prior $p(\\mathbf{z}) \\sim N(0,I)$ and then use these samples to generate images using the decoder.\n","\n","<figure>\n","<center>\n","<img src='https://miro.medium.com/max/1400/1*Q5dogodt3wzKKktE0v3dMQ@2x.png' width=\"500\" height=\"300\"/>\n","<figcaption>Variational Autoencoder Illustration</figcaption></center>\n","</figure>\n","\n","**Generative Adversarial Networks**: GANs are deep generative models which use a different strategy than VAEs for training. Similar to VAEs, GANs also have a generator which receives a random vector $\\mathbf{z} \\sim N(0,I)$ to generate a synthectic image $\\mathbf{x}_{gen}$. However, instead of using an encoder , GANs are trained employing a binary classifier implemented with a neural network. This binary classifier is known as the discriminator and it tries to distinguish between real samples $\\mathbf{x}$ and generated ones $\\mathbf{x}_{gen}$.\n","<figure>\n","<center>\n","<img src='https://www.researchgate.net/profile/Emiliano_De_Cristofaro/publication/317061929/figure/fig1/AS:497029693362176@1495512521469/Generative-Adversarial-Network-GAN.png' width=\"400\" height=\"200\"/>\n","<figcaption>Image Caption</figcaption></center>\n","</figure>\n","\n","The generator and the discriminator are trained by playing a \"game\" where they compete between each other. The generator try to \"fool\" the discriminator generating realistic samples, whereas the discriminator attempts to detect what images are real or generated. In particular, the networks maximize the following losses:\n","\n","$L_{Disc} = \\sum_{\\mathbf{x}_{gen} \\sim Gen(\\mathbf{z})} \\log(1-Disc(\\mathbf{x}_{gen})) + \\sum_{\\mathbf{x}_{real}} Disc(\\mathbf{x}_{real})$\n","\n","$L_{Gen} = \\sum_{\\mathbf{x}_{gen} \\sim Gen(\\mathbf{z})} \\log(Disc(\\mathbf{x}_{gen}))$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j4-rWsMDzzZj"},"source":["## Example of Variational Autoencoder on MNIST\n","\n","In the following, we will implement, train and evaluate a VAE on the MNIST dataset wit 32x32 images.\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gCvL52Zp41XU"},"source":["### Encoder/Decoder definitions\n","\n","Let's define the encoder and decoder networks:\n","\n","(1) The encoder is composed of different blocks composed of a convolution + batch normalization + ReLU layers. Additionally, average pooling is used to reduce the spatial resolution at the end of each block. After these blocks, a fully-connected layer is used to compute two vectors representing the mean $\\mu_x$ and the logarithm of the variance $\\sigma^2$ of the variable $\\mathbf{z}$ for a given sample $\\mathbf{x}$\n","\n","(2) The decoder is used to transform a vector $\\mathbf{z}$ to an image $\\mathbf{x}$. It is composed of an initial fully connected layer and a set of batch normalization +  ReLU + convolution blocks. Nearest neighbour upsampling is used to  progressevely increase the spatial resolution of the tensors. A final sigmoid function is used to normalize the output pixels values between 0 and 1."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4620,"status":"ok","timestamp":1685212196566,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"ssNaBKJXrnbH"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# Convolution + BatchNormnalization + ReLU block for the encoder\n","class ConvBNReLU(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(ConvBNReLU, self).__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.AvgPool2d(2,2)\n","\n","  def forward(self,x):\n","    if(self.pool):\n","      out = self.pool(x)\n","    else:\n","      out = x\n","    out = self.relu(self.bn(self.conv(out)))   \n","    return out\n","\n","#  BatchNormnalization + ReLU block + Convolution for the decoder\n","class BNReLUConv(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(BNReLUConv, self).__init__()\n","    self.bn = nn.BatchNorm2d(in_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.UpsamplingNearest2d(scale_factor=2)\n","\n","  def forward(self,x):\n","    out = self.relu(self.bn(x))\n","    if(self.pool):\n","      out = self.pool(out)\n","    out = self.conv(out)\n","    return out\n","\n","# Encoder definition with 3 COnv-BN-ReLU blocks and fully-connected layer\n","class Encoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Encoder, self).__init__()\n","    self.layer1 = ConvBNReLU(1,base_channels,pooling=False)\n","    self.layer2 = ConvBNReLU(base_channels,base_channels*2,pooling=True)\n","    self.layer3 = ConvBNReLU(base_channels*2,base_channels*4,pooling=True)\n","    self.fc = nn.Linear(8*8*base_channels*4,out_features)\n","  \n","  def forward(self,x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    return self.fc(out.view(x.shape[0],-1))\n","    \n","# Decoder definition with a fully-connected layer and 3 BN-ReLU-COnv blocks and \n","class Decoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Decoder, self).__init__()\n","    self.base_channels = base_channels\n","    self.fc = nn.Linear(out_features,8*8*base_channels*4)\n","    self.layer3 = BNReLUConv(base_channels*4,base_channels*2,pooling=True)\n","    self.layer2 = BNReLUConv(base_channels*2,base_channels,pooling=True)\n","    self.layer1 = BNReLUConv(base_channels,1,pooling=False)\n","  \n","  def forward(self,x):\n","    out = self.fc(x)\n","    out = out.view(x.shape[0],self.base_channels*4,8,8)\n","    out = self.layer3(out)\n","    out = self.layer2(out)\n","    out = self.layer1(out)\n","    return torch.sigmoid(out)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p_dX-bV827Ri"},"source":["### Autoencoder Definition\n","\n","We use the Encoder and Decoder modules to create a VAE class."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685212196566,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"veedOT0K5BdE","outputId":"f6de4b7d-b220-41d3-ddcd-a1ec3d3905f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["MNIST VAE Definition\n","VAE(\n","  (encoder): Encoder(\n","    (layer1): ConvBNReLU(\n","      (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (layer2): ConvBNReLU(\n","      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (layer3): ConvBNReLU(\n","      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (fc): Linear(in_features=4096, out_features=64, bias=True)\n","  )\n","  (decoder): Decoder(\n","    (fc): Linear(in_features=32, out_features=4096, bias=True)\n","    (layer3): BNReLUConv(\n","      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (pool): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n","    )\n","    (layer2): BNReLUConv(\n","      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (pool): UpsamplingNearest2d(scale_factor=2.0, mode='nearest')\n","    )\n","    (layer1): BNReLUConv(\n","      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n",")\n"]}],"source":["class VAE(nn.Module):\n","  def __init__(self, out_features=32,base_channels=16):\n","    super(VAE, self).__init__()\n","    # Initialize the encoder and decoder using a dimensionality out_features for the vector z\n","    self.out_features = out_features\n","    self.encoder = Encoder(out_features*2,base_channels)\n","    self.decoder = Decoder(out_features,base_channels)\n","\n","  # function to obtain the mu and sigma of z for a samples x\n","  def encode(self,x):\n","    aux = self.encoder(x)\n","    # get z mean\n","    z_mean = aux[:,0:self.out_features]\n","    # get z variance\n","    z_log_var = aux[:,self.out_features::]\n","    return z_mean, z_log_var\n","\n","  # function to generate a random sample z given mu and sigma\n","  def sample_z(self,z_mean,z_log_var):\n","    z_std = z_log_var.mul(0.5).exp() # 0.5 is the error\n","    samples_unit_normal = torch.randn_like(z_mean)\n","    samples_z = samples_unit_normal*z_std + z_mean\n","    return samples_z\n","\n","  # (1) encode a sample\n","  # (2) obtain a random vector z from mu and sigma\n","  # (3) Reconstruct the image using the decoder\n","  def forward(self,x):\n","    z_mean, z_log_var = self.encode(x)\n","    samples_z = self.sample_z(z_mean,z_log_var)\n","    x_rec = self.decoder(samples_z)\n","    return x_rec, z_mean, z_log_var\n","\n","# Print summary of the mode\n","print('MNIST VAE Definition')\n","vae = VAE(32)\n","print(vae)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GWNGMOjl8GQ9"},"source":["### MNIST Dataset Loaders\n","\n","Simple MNIST data loader definition"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52750,"status":"ok","timestamp":1685212249315,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"5kE5D0hh1Ndz","outputId":"377ad3a4-0499-45d7-af8e-153a9ef3dce8"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\eduard\\uni\\3er\\trim3\\DeepLearning\\DeepLearning\\P4\\P4-Examples.ipynb Cell 9\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/eduard/uni/3er/trim3/DeepLearning/DeepLearning/P4/P4-Examples.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/eduard/uni/3er/trim3/DeepLearning/DeepLearning/P4/P4-Examples.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/eduard/uni/3er/trim3/DeepLearning/DeepLearning/P4/P4-Examples.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/eduard/uni/3er/trim3/DeepLearning/DeepLearning/P4/P4-Examples.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Load an image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/eduard/uni/3er/trim3/DeepLearning/DeepLearning/P4/P4-Examples.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import matplotlib.pyplot as plt\n","import PIL\n","from google.colab import drive\n","\n","\n","# Load an image\n","drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/DeepLearning_2023/P4/Data/'\n","results_path = '/content/drive/My Drive/DeepLearning_2023/P4/Results/'\n","\n","#Making native class loader\n","class MNIST(torch.utils.data.Dataset):\n","    # Initialization method for the dataset\n","    def __init__(self,dataDir = data_path+'/mnist/train.pt',transform = None):\n","        self.data, self.labels = torch.load(dataDir)\n","        self.transform = transform\n","     # What to do to load a single item in the dataset ( read image and label)    \n","    def __getitem__(self, index):\n","        data = self.data[index]\n","        lbl = self.labels[index]\n","        \n","        data = PIL.Image.fromarray(data.numpy(), mode='L')\n","        # Apply a trasnformaiton to the image if it is indicated in the initalizer\n","        if self.transform is not None : \n","            data = self.transform(data)\n","        \n","        # return the image and the label\n","        return data,lbl\n","    \n","        pass\n","    \n","    # Return the number of images\n","    def __len__(self):\n","        return len(self.data)\n","\n","tr = tf.Compose([\n","        tf.Resize((32,32)),\n","        tf.ToTensor(), # convert image to pytorch tensor [0..,1]\n","        ])\n","\n","\n","# Initialize the dataset\n","MNISTTrain = MNIST(data_path+'/mnist/train.pt',tr)\n","\n","# Class to iterate over the dataset (DataLoader)\n","train_loader = torch.utils.data.DataLoader(dataset=MNISTTrain, # idnicate the used dataset\n","                                           batch_size=128, # Number of images that will be loaded for iteration\n","                                           shuffle=True) # Sequential or random data loading\n","MNISTTest = MNIST(data_path+'/mnist/test.pt',tr)\n","test_loader = torch.utils.data.DataLoader(dataset=MNISTTest,\n","                                          batch_size=64, \n","                                          shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kKUX-xzcMNsX"},"source":["### Train functions VAE"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685212249316,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"dqfJRZ8lMRe9"},"outputs":[],"source":["## Kullback-Leibler regularization computation\n","def kl_divergence(z_mean,z_log_var):\n","  kl_loss = 0.5 * torch.sum(  (torch.exp(z_log_var) + z_mean**2 - 1.0 - z_log_var),axis=1)\n","  return kl_loss.mean()\n","\n","# Train function\n","def train_VAE(vae,  train_loader, optimizer, kl_weight=0.001, num_epochs=10, model_name='vae_mnist.ckpt', device='cpu'):\n","    vae.to(device)\n","    vae.train() # Set the model in train mode\n","    total_step = len(train_loader)\n","    losses_list = []\n","    criterion = nn.MSELoss() # Use mean-squared error to compare the original and reconstructe images\n","    \n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        rec_loss_avg = 0\n","        kl_loss_avg = 0\n","        nBatches = 0\n","        for i, (images, labels) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            images = images.to(device)\n","\n","            # Forward pass (get encoder variables and reconstructed images)\n","            x_rec, z_mean, z_log_var = vae(images)\n","            print(z_mean.shape)\n","            reconstruction_loss = criterion(x_rec, images) # Reconstruction loss (x,x_rec)\n","            kl_loss = kl_divergence(z_mean, z_log_var) # Compute KL divergecnes KL( N(mu_x,sigma_x) || N(0,I))\n","            \n","            # Backward and optimize reconstruction loss and kl regularization\n","            optimizer.zero_grad()\n","            loss = reconstruction_loss + kl_loss*kl_weight # we use a weight to balance the importance of the KL loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            rec_loss_avg += reconstruction_loss.cpu().item()\n","            kl_loss_avg += kl_loss.cpu().item()\n","\n","            nBatches+=1\n","            if (i+1) % 100 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        losses_list.append(rec_loss_avg / nBatches)\n","        # save trained model\n","        torch.save(vae.state_dict(), results_path+ '/' + model_name)\n","          \n","    return losses_list "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZYWhH1BAPLtL"},"source":["### Train VAE"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378340,"status":"ok","timestamp":1685212627653,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"SmgxD-jtPLT2","outputId":"38fb2f0e-6506-4a61-9867-9515984be780"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/20], Step [100/469], Rec. Loss: 0.0625, KL Loss: 9.4696\n","Epoch [1/20], Step [200/469], Rec. Loss: 0.0450, KL Loss: 10.8389\n","Epoch [1/20], Step [300/469], Rec. Loss: 0.0377, KL Loss: 11.6221\n","Epoch [1/20], Step [400/469], Rec. Loss: 0.0335, KL Loss: 12.0687\n","Epoch [1/20], Step [469/469], Rec. Loss: 0.0315, KL Loss: 12.2630\n","Epoch [2/20], Step [100/469], Rec. Loss: 0.0190, KL Loss: 13.3679\n","Epoch [2/20], Step [200/469], Rec. Loss: 0.0188, KL Loss: 13.3391\n","Epoch [2/20], Step [300/469], Rec. Loss: 0.0185, KL Loss: 13.3446\n","Epoch [2/20], Step [400/469], Rec. Loss: 0.0182, KL Loss: 13.3154\n","Epoch [2/20], Step [469/469], Rec. Loss: 0.0181, KL Loss: 13.3119\n","Epoch [3/20], Step [100/469], Rec. Loss: 0.0169, KL Loss: 13.2845\n","Epoch [3/20], Step [200/469], Rec. Loss: 0.0168, KL Loss: 13.2615\n","Epoch [3/20], Step [300/469], Rec. Loss: 0.0167, KL Loss: 13.2314\n","Epoch [3/20], Step [400/469], Rec. Loss: 0.0166, KL Loss: 13.2391\n","Epoch [3/20], Step [469/469], Rec. Loss: 0.0166, KL Loss: 13.2336\n","Epoch [4/20], Step [100/469], Rec. Loss: 0.0160, KL Loss: 13.0971\n","Epoch [4/20], Step [200/469], Rec. Loss: 0.0160, KL Loss: 13.1014\n","Epoch [4/20], Step [300/469], Rec. Loss: 0.0160, KL Loss: 13.0964\n","Epoch [4/20], Step [400/469], Rec. Loss: 0.0160, KL Loss: 13.0824\n","Epoch [4/20], Step [469/469], Rec. Loss: 0.0159, KL Loss: 13.0818\n","Epoch [5/20], Step [100/469], Rec. Loss: 0.0156, KL Loss: 13.0353\n","Epoch [5/20], Step [200/469], Rec. Loss: 0.0156, KL Loss: 13.0415\n","Epoch [5/20], Step [300/469], Rec. Loss: 0.0155, KL Loss: 13.0140\n","Epoch [5/20], Step [400/469], Rec. Loss: 0.0155, KL Loss: 13.0072\n","Epoch [5/20], Step [469/469], Rec. Loss: 0.0155, KL Loss: 13.0058\n","Epoch [6/20], Step [100/469], Rec. Loss: 0.0153, KL Loss: 12.9881\n","Epoch [6/20], Step [200/469], Rec. Loss: 0.0153, KL Loss: 12.9923\n","Epoch [6/20], Step [300/469], Rec. Loss: 0.0152, KL Loss: 12.9679\n","Epoch [6/20], Step [400/469], Rec. Loss: 0.0152, KL Loss: 12.9552\n","Epoch [6/20], Step [469/469], Rec. Loss: 0.0152, KL Loss: 12.9489\n","Epoch [7/20], Step [100/469], Rec. Loss: 0.0150, KL Loss: 12.8900\n","Epoch [7/20], Step [200/469], Rec. Loss: 0.0150, KL Loss: 12.9279\n","Epoch [7/20], Step [300/469], Rec. Loss: 0.0150, KL Loss: 12.9161\n","Epoch [7/20], Step [400/469], Rec. Loss: 0.0150, KL Loss: 12.9241\n","Epoch [7/20], Step [469/469], Rec. Loss: 0.0150, KL Loss: 12.9085\n","Epoch [8/20], Step [100/469], Rec. Loss: 0.0149, KL Loss: 12.8049\n","Epoch [8/20], Step [200/469], Rec. Loss: 0.0149, KL Loss: 12.8318\n","Epoch [8/20], Step [300/469], Rec. Loss: 0.0149, KL Loss: 12.8283\n","Epoch [8/20], Step [400/469], Rec. Loss: 0.0149, KL Loss: 12.8305\n","Epoch [8/20], Step [469/469], Rec. Loss: 0.0149, KL Loss: 12.8264\n","Epoch [9/20], Step [100/469], Rec. Loss: 0.0148, KL Loss: 12.7597\n","Epoch [9/20], Step [200/469], Rec. Loss: 0.0148, KL Loss: 12.7904\n","Epoch [9/20], Step [300/469], Rec. Loss: 0.0148, KL Loss: 12.7670\n","Epoch [9/20], Step [400/469], Rec. Loss: 0.0148, KL Loss: 12.7914\n","Epoch [9/20], Step [469/469], Rec. Loss: 0.0148, KL Loss: 12.7820\n","Epoch [10/20], Step [100/469], Rec. Loss: 0.0146, KL Loss: 12.7513\n","Epoch [10/20], Step [200/469], Rec. Loss: 0.0147, KL Loss: 12.7599\n","Epoch [10/20], Step [300/469], Rec. Loss: 0.0147, KL Loss: 12.7593\n","Epoch [10/20], Step [400/469], Rec. Loss: 0.0147, KL Loss: 12.7545\n","Epoch [10/20], Step [469/469], Rec. Loss: 0.0147, KL Loss: 12.7576\n","Epoch [11/20], Step [100/469], Rec. Loss: 0.0146, KL Loss: 12.7320\n","Epoch [11/20], Step [200/469], Rec. Loss: 0.0146, KL Loss: 12.7480\n","Epoch [11/20], Step [300/469], Rec. Loss: 0.0146, KL Loss: 12.7392\n","Epoch [11/20], Step [400/469], Rec. Loss: 0.0146, KL Loss: 12.7376\n","Epoch [11/20], Step [469/469], Rec. Loss: 0.0146, KL Loss: 12.7362\n","Epoch [12/20], Step [100/469], Rec. Loss: 0.0145, KL Loss: 12.6517\n","Epoch [12/20], Step [200/469], Rec. Loss: 0.0145, KL Loss: 12.6707\n","Epoch [12/20], Step [300/469], Rec. Loss: 0.0145, KL Loss: 12.6756\n","Epoch [12/20], Step [400/469], Rec. Loss: 0.0145, KL Loss: 12.6887\n","Epoch [12/20], Step [469/469], Rec. Loss: 0.0145, KL Loss: 12.6894\n","Epoch [13/20], Step [100/469], Rec. Loss: 0.0144, KL Loss: 12.7002\n","Epoch [13/20], Step [200/469], Rec. Loss: 0.0144, KL Loss: 12.7082\n","Epoch [13/20], Step [300/469], Rec. Loss: 0.0144, KL Loss: 12.7084\n","Epoch [13/20], Step [400/469], Rec. Loss: 0.0144, KL Loss: 12.6834\n","Epoch [13/20], Step [469/469], Rec. Loss: 0.0144, KL Loss: 12.6828\n","Epoch [14/20], Step [100/469], Rec. Loss: 0.0144, KL Loss: 12.6683\n","Epoch [14/20], Step [200/469], Rec. Loss: 0.0144, KL Loss: 12.6890\n","Epoch [14/20], Step [300/469], Rec. Loss: 0.0143, KL Loss: 12.6717\n","Epoch [14/20], Step [400/469], Rec. Loss: 0.0144, KL Loss: 12.6701\n","Epoch [14/20], Step [469/469], Rec. Loss: 0.0143, KL Loss: 12.6661\n","Epoch [15/20], Step [100/469], Rec. Loss: 0.0144, KL Loss: 12.6050\n","Epoch [15/20], Step [200/469], Rec. Loss: 0.0144, KL Loss: 12.6248\n","Epoch [15/20], Step [300/469], Rec. Loss: 0.0143, KL Loss: 12.6287\n","Epoch [15/20], Step [400/469], Rec. Loss: 0.0143, KL Loss: 12.6303\n","Epoch [15/20], Step [469/469], Rec. Loss: 0.0143, KL Loss: 12.6366\n","Epoch [16/20], Step [100/469], Rec. Loss: 0.0142, KL Loss: 12.6364\n","Epoch [16/20], Step [200/469], Rec. Loss: 0.0142, KL Loss: 12.6544\n","Epoch [16/20], Step [300/469], Rec. Loss: 0.0142, KL Loss: 12.6377\n","Epoch [16/20], Step [400/469], Rec. Loss: 0.0142, KL Loss: 12.6347\n","Epoch [16/20], Step [469/469], Rec. Loss: 0.0142, KL Loss: 12.6200\n","Epoch [17/20], Step [100/469], Rec. Loss: 0.0143, KL Loss: 12.6133\n","Epoch [17/20], Step [200/469], Rec. Loss: 0.0142, KL Loss: 12.5969\n","Epoch [17/20], Step [300/469], Rec. Loss: 0.0142, KL Loss: 12.5761\n","Epoch [17/20], Step [400/469], Rec. Loss: 0.0142, KL Loss: 12.5806\n","Epoch [17/20], Step [469/469], Rec. Loss: 0.0142, KL Loss: 12.5842\n","Epoch [18/20], Step [100/469], Rec. Loss: 0.0142, KL Loss: 12.5738\n","Epoch [18/20], Step [200/469], Rec. Loss: 0.0142, KL Loss: 12.5969\n","Epoch [18/20], Step [300/469], Rec. Loss: 0.0142, KL Loss: 12.5799\n","Epoch [18/20], Step [400/469], Rec. Loss: 0.0142, KL Loss: 12.5773\n","Epoch [18/20], Step [469/469], Rec. Loss: 0.0142, KL Loss: 12.5864\n","Epoch [19/20], Step [100/469], Rec. Loss: 0.0142, KL Loss: 12.5565\n","Epoch [19/20], Step [200/469], Rec. Loss: 0.0142, KL Loss: 12.5606\n","Epoch [19/20], Step [300/469], Rec. Loss: 0.0141, KL Loss: 12.5737\n","Epoch [19/20], Step [400/469], Rec. Loss: 0.0141, KL Loss: 12.5743\n","Epoch [19/20], Step [469/469], Rec. Loss: 0.0141, KL Loss: 12.5813\n","Epoch [20/20], Step [100/469], Rec. Loss: 0.0141, KL Loss: 12.5597\n","Epoch [20/20], Step [200/469], Rec. Loss: 0.0141, KL Loss: 12.5860\n","Epoch [20/20], Step [300/469], Rec. Loss: 0.0141, KL Loss: 12.5923\n","Epoch [20/20], Step [400/469], Rec. Loss: 0.0141, KL Loss: 12.5831\n","Epoch [20/20], Step [469/469], Rec. Loss: 0.0141, KL Loss: 12.5690\n"]}],"source":["# Training a VAE on MNIST: z has 32 dimensions\n","# We use Adam optimizer which is tipically used in VAEs and GANs\n","\n","vae = VAE(32)\n","kl_weight=0.001 \n","\n","#Initialize optimizer \n","learning_rate = .001\n","optimizer = torch.optim.Adam(vae.parameters(),lr = learning_rate, weight_decay=1e-5)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","loss_list = train_VAE(vae, train_loader, optimizer, kl_weight=kl_weight,\n","                      num_epochs=20, model_name='vae_mnist.ckpt', device=device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X0E6EMHBUxtk"},"source":["### Visualize Reconstructed Images and compute reconstruction error in test set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1G8CeMiHWOGKDWnioBG7nTU73i233bJc1"},"id":"2EDYWZF54R7M","outputId":"aea2e0e8-a06e-40f6-b9e8-d868290ff73d"},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","### Generate random samples\n","vae.eval()\n","# 64 interpolations\n","n_samples = 64\n","n_iterpolations =50\n","\n","# Sample a set of pairs z_init and z_final\n","z_init = torch.randn((n_samples,vae.out_features)).to(device)*2\n","z_final = torch.randn((n_samples,vae.out_features)).to(device)*2\n","\n","# Compute interpolations between z_init and z_final\n","# and generate an image for each interpolation.\n","interpolation_images = []\n","for interp in range(0,n_iterpolations):\n","  interp_0_1 = float(interp) / (n_iterpolations-1)\n","  z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","  x_rec = vae.decoder(z)\n","  image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","  image_grid = image_grid.permute(1,2,0).detach().numpy()\n","  # save the generated images in a list\n","  interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","\n","# Concatenate the inversion of the list to generate a \"loop\" animation\n","interpolation_images += interpolation_images[::-1]\n","\n","# Generate and visualize a give showing the interpolation results.\n","imname = results_path+'/vae_interpolation_mnist.gif'\n","imageio.mimsave(imname, interpolation_images, fps=25)\n","\n","with open(imname,'rb') as f:\n","    display(Image(data=f.read(), format='png',width=512,height=512))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":734},"executionInfo":{"elapsed":1612,"status":"ok","timestamp":1685212629255,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"sxpvEjaIU14o","outputId":"f447596a-70b7-4eff-941f-7c234a1e7372"},"outputs":[],"source":["from torchvision.utils import make_grid\n","import matplotlib.pyplot as plt\n","\n","# Load trained VAE\n","vae = VAE(32)\n","vae.eval() # Evaluation mode for the model\n","vae.load_state_dict(torch.load(results_path+'/vae_mnist.ckpt')) # Load model\n","vae = vae.to(device)\n","\n","# Visualize reconstructions on test samples\n","\n","test_images,_ = next(iter(test_loader))\n","test_images = test_images.to(device)\n","# Get reconstructed test images with the VAE\n","_, z_mean, _ = vae(test_images)\n","x_rec = vae.decoder(z_mean)\n","\n","plt.figure(figsize=(18,9))\n","image_grid = make_grid(test_images.cpu(),nrow=8,padding=1)\n","plt.subplot(1,2,1)\n","plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","plt.title('Original Images')\n","\n","plt.subplot(1,2,2)\n","plt.title('Reconstructed Images')\n","image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","plt.show()\n","\n","# Compute reconstruction loss on testing dataset\n","rec_loss_avg = 0\n","criterion = nn.MSELoss()\n","n_batches = 0 \n","for i, (images, labels) in enumerate(test_loader):\n","  # Get batch of samples and labels\n","  images = images.to(device)\n","\n","  # Forward pass\n","  z_mean, z_log_var = vae.encode(images)\n","  x_rec = vae.decoder(z_mean)\n","  reconstruction_loss = criterion(x_rec, images)\n","  rec_loss_avg += reconstruction_loss.cpu().item()\n","  n_batches += 1\n","print('Reconstruction Error on test set: ' + str(rec_loss_avg / n_batches))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JT___MNn3-xu"},"source":["### Visualize Randomly Generated Images\n","\n","Now we will sample random vectors $\\mathbf{z} \\sim N(0,I)$ and create sythetic images with our trained decoder. Note that the generated images are not reconstructed from real images and, thus, they do not belong to the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":793},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685212629255,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"xkqJIlKxpilY","outputId":"e0083fc2-3267-42c2-c474-ee57bccc90e8"},"outputs":[],"source":["### Generate random samples\n","n_samples = 64\n","vae.eval()\n","# Random vectors z~N(0,I)\n","z = torch.randn((n_samples,vae.out_features)).to(device)\n","\n","# Genearte images with the decoder from the random vectors\n","x_rec = vae.decoder(z)\n","\n","# Show synthetic images\n","plt.figure(figsize=(9,9))\n","plt.title('Generated Images')\n","image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qX42Dlvv4NRL"},"source":["### Interpolation in the latent space\n","\n","Another way to visualize VAE results is to perform interpolations in the latent space of vectors $\\mathbf{z}$. In particular, we will generate different two different random vectors $\\mathbf{z}_{init}$ and $\\mathbf{z}_{final}$ and generate multiple images decoding the interpolations between them."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vUrqAof5JRjH"},"source":["## Example of Generative Adversarial Networks on MNIST\n","\n","In the following we will implement, train and evaluate a GAN in MNIST dataset. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"solYuHIIbWWs"},"source":["### GAN Definition\n","\n","(1) The generator network will have the same definition than our VAE decoder\n","(2) The discriminator will have a similar definition than our VAE encoder. The difference is that the last fully-connected layer will be a binary classifier which will output a single value representing the probability of fake/real image."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kOGfocqrMOZp"},"outputs":[],"source":["# Discriminator similar to VAE encoder\n","class Discriminator(nn.Module):\n","  def __init__(self, base_channels=16):\n","    super(Discriminator, self).__init__()\n","    # last fully connected layer acts as a a binary classifier\n","    self.classifier = Encoder(1,base_channels)\n","\n","  # Forward pass obtaining the discriminator probability\n","  def forward(self,x):\n","    out = self.classifier(x)\n","    # use sigmoid to get the real/fake image probability\n","    return torch.sigmoid(out)\n","\n","# Generator is defined as VAE decoder\n","class Generator(nn.Module):\n","  def __init__(self,in_features,base_channels=16):\n","    super(Generator, self).__init__()\n","    self.base_channels = base_channels\n","    self.in_features = in_features\n","    self.decoder = Decoder(in_features,base_channels)\n","\n","  # Generate an image from vector z\n","  def forward(self,z):\n","    return torch.sigmoid(self.decoder(z))\n","\n","  # Sample a set of images from random vectors z\n","  def sample(self,n_samples=256,device='cpu'):\n","    samples_unit_normal = torch.randn((n_samples,self.in_features)).to(device)\n","    return self.decoder(samples_unit_normal)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LGcNukaXbPrv"},"source":["### GAN Train Function\n","\n","Definition of the training process for our GAN. Generator and discriminator losses are jointly optimized."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"grUmuf9WkQmV"},"outputs":[],"source":["# GAN Train function. We have a generator and discriminator models and their respective optimizers.\n","def train_GAN(gen, disc,  train_loader, optimizer_gen, optim_disc,\n","              num_epochs=10, model_name='gan_mnist.ckpt', device='cpu'):\n","    gen = gen.to(device)\n","    gen.train() # Set the generator in train mode\n","    disc = disc.to(device)\n","    disc.train() # Set the discriminator in train mode\n","\n","    total_step = len(train_loader)\n","    losses_list = []\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        disc_loss_avg = 0\n","        gen_loss_avg = 0\n","        nBatches = 0\n","        update_generator = True\n","\n","        for i, (real_images, labels) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            real_images = real_images.to(device)\n","            n_images = real_images.shape[0]\n","\n","            # Forward pass\n","            # Generate random images with the generator\n","            fake_images = gen.sample(n_images,device=device)\n","            \n","            # Use the discriminator to obtain the probabilties for real and generate imee\n","            prob_real = disc(real_images)\n","            prob_fake = disc(fake_images)\n","            \n","            # Generator loss\n","            gen_loss = -torch.log(prob_fake).mean()\n","            # Discriminator loss\n","            disc_loss = -0.5*(torch.log(prob_real) + torch.log(1-prob_fake)).mean()\n","\n","            \n","            # We are going to update the discriminator and generator parameters alternatively at each iteration\n","\n","            if(update_generator):\n","              # Optimize generator\n","              # Backward and optimize\n","              optimizer_gen.zero_grad()\n","              gen_loss.backward() # Necessary to not erase intermediate variables needed for computing disc_loss gradient\n","              optimizer_gen.step()\n","              update_generator = False\n","            else:           \n","              # Optimize discriminator\n","              # Backward and optimize\n","              optimizer_disc.zero_grad()\n","              disc_loss.backward()\n","              optimizer_disc.step()\n","              update_generator = True\n","                \n","\n","            disc_loss_avg += disc_loss.cpu().item()\n","            gen_loss_avg += gen_loss.cpu().item()\n","\n","            nBatches+=1\n","            if (i+1) % 200 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Gen. Loss: {:.4f}, Disc Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, gen_loss_avg / nBatches, disc_loss_avg / nBatches))\n","        # Save model\n","        losses_list.append(disc_loss_avg / nBatches)\n","        torch.save(gan_gen.state_dict(), results_path+ '/' + model_name)\n","          \n","    return losses_list "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VSnl4ECjqTjG"},"source":["### Trainning a GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PgsRY45bqWzK"},"outputs":[],"source":["# Define Geneartor and Discriminator networks\n","gan_gen = Generator(32)\n","gan_disc = Discriminator()\n","\n","#Initialize indepdent optimizer for both networks\n","learning_rate = .0005\n","optimizer_gen = torch.optim.Adam(gan_gen.parameters(),lr = learning_rate, weight_decay=1e-5)\n","optimizer_disc = torch.optim.Adam(gan_disc.parameters(),lr = learning_rate, weight_decay=1e-5)\n","\n","# Train the GAN\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","loss_list = train_GAN(gan_gen,gan_disc, train_loader, optimizer_gen, optimizer_disc,\n","                      num_epochs=20, model_name='gan_mnist.ckpt', device=device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0PwOwPj-pFK3"},"source":["### Visualize synthetic images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ny1pq0vrso6"},"outputs":[],"source":["# Load generator\n","gan_gen = Generator(32)\n","gan_gen.load_state_dict(torch.load(results_path+'/gan_mnist.ckpt'))\n","gan_gen.eval() # Put in eval model\n","gan_gen = gan_gen.to(device)\n","\n","# Generate random images from sampled vectors z and visualize them \n","x_gen = gan_gen.sample(64,device=device)\n","image_grid = make_grid(x_gen.cpu(),nrow=8,padding=1)\n","plt.figure(figsize=(8,8))\n","plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mk380xhKpSwW"},"source":["### Visualize latent space interpolations\n","\n","Similar to VAEs we can also visualize images generated from z interpolations in GANs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1FZjR9BPMFXZrGWnZuD8D-llM_mlM5vHc"},"id":"DsiiayrD39E1","outputId":"8628e9f9-21c7-4884-9e44-8b83e787c8bb"},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","### Generate random samples\n","gan_gen = Generator(32)\n","gan_gen.load_state_dict(torch.load(results_path+'/gan_mnist.ckpt'))\n","gan_gen = gan_gen.to(device)\n","gan_gen.eval()\n","\n","\n","n_samples = 64\n","n_iterpolations =50\n","\n","z_init = torch.randn((n_samples,vae.out_features)).to(device)\n","z_final = torch.randn((n_samples,vae.out_features)).to(device)\n","\n","interpolation_images = []\n","for interp in range(0,n_iterpolations):\n","  interp_0_1 = float(interp) / (n_iterpolations-1)\n","  z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","  x_rec = gan_gen.decoder(z.to(device))\n","  image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","  image_grid = image_grid.permute(1,2,0).detach().numpy()\n","\n","  interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","interpolation_images += interpolation_images[::-1]\n","\n","imname = results_path+'/gan_interpolation_mnist.gif'\n","imageio.mimsave(imname, interpolation_images, fps=25)\n","\n","with open(imname,'rb') as f:\n","    display(Image(data=f.read(), format='png',width=512,height=512))"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
