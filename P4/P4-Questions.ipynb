{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"IvcOz1eTrxRx"},"source":["# **Deep Generative Models: Variational Autoencoders and Generative Adversarial Networks**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EvWs2XIYNfhg"},"source":["# **CK Dataset**\n","In the following exercices, you will work with images extracted from the CK dataset: http://www.jeffcohn.net/wp-content/uploads/2020/02/Cohn-Kanade_Database.pdf.pdf\n","\n","It contains gray-scale images of human faces.\n","\n","The dataset is provided in the folder Data/faces/ in .mat format.\n","In the following we provide a Dataset class in pytorch to load images from this database."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"executionInfo":{"elapsed":43049,"status":"ok","timestamp":1653800688560,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"5kE5D0hh1Ndz","outputId":"9483add6-4ad6-45c9-f5df-a2326394b6d9"},"outputs":[],"source":["## Create a Custom Dataset for CK database\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import os\n","import time\n","\n","# Mount Google Drive\n","\n","try:\n","    print(\"Currently on path\", base_path)\n","except NameError:\n","    base_path = os.getcwd().replace('\\\\', '/')\n","    print(\"Path is not defined, initialized to\", base_path)\n","\n","data_path = base_path+'/Data/'\n","results_path = base_path+'/Results/'\n","\n","# All the data will be loaded from the provided file in Data/mnist.t\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import scipy.io as sio\n","from torchvision.utils import make_grid\n","\n","\n","\n","#Making native class loader\n","class FacesDB(torch.utils.data.Dataset):\n","    # Initialization method for the dataset\n","    def __init__(self,dataDir = data_path+'/faces/face_ims_64x64.mat', transform = None):\n","        mat_loaded = sio.loadmat(dataDir)\n","        self.data = mat_loaded['X']\n","        self.transform = transform\n","\n","    # What to do to load a single item in the dataset ( read image and label)    \n","    def __getitem__(self, index):\n","        data = self.data[:,:,0,index]   \n","        data = Image.fromarray(data,mode='L')\n","        # Apply a trasnformaiton to the image if it is indicated in the initalizer\n","        if self.transform is not None : \n","            data = self.transform(data)\n","        \n","        # return the image and the label\n","        return data\n","\n","    # Return the number of images\n","    def __len__(self):\n","        return self.data.shape[3]\n","\n","import torchvision.transforms as transforms\n","\n","tr = transforms.Compose([\n","        transforms.ToTensor(), \n","        ])\n","faces_db = FacesDB(data_path+'/faces/face_ims_64x64.mat',tr)\n","train_loader = torch.utils.data.DataLoader(dataset=faces_db,\n","                                           batch_size=256, \n","                                           shuffle=True)\n","\n","# Mini-batch images\n","images = next(iter(train_loader))\n","print(images.shape)\n","image = images[0,:,:,:].repeat(3,1,1)\n","plt.imshow(image.permute(1,2,0).squeeze().numpy())\n","plt.axis('off')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CVA6m-IgNace"},"source":["# Ex. 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1653800688561,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"y1yTECVjDVmf","outputId":"5285d823-5808-45c8-bbd9-c18510b02767"},"outputs":[],"source":["'''\n","1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# Convolution + BatchNormnalization + ReLU block for the encoder\n","class ConvBNReLU(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(ConvBNReLU, self).__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.AvgPool2d(2,2)\n","\n","  def forward(self,x):\n","    if(self.pool):\n","      out = self.pool(x)\n","    else:\n","      out = x\n","    out = self.relu(self.bn(self.conv(out)))   \n","    return out\n","\n","#  BatchNormnalization + ReLU block + Convolution for the decoder\n","class BNReLUConv(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(BNReLUConv, self).__init__()\n","    self.bn = nn.BatchNorm2d(in_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.UpsamplingNearest2d(scale_factor=2)\n","\n","  def forward(self,x):\n","    out = self.relu(self.bn(x))\n","    if(self.pool):\n","      out = self.pool(out)\n","    out = self.conv(out)\n","    return out\n","\n","# Encoder definition with 3 COnv-BN-ReLU blocks and fully-connected layer\n","class Encoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Encoder, self).__init__()\n","    self.layer1 = ConvBNReLU(1,base_channels,pooling=False)\n","    self.layer2 = ConvBNReLU(base_channels,base_channels*2,pooling=True)\n","    self.layer3 = ConvBNReLU(base_channels*2,base_channels*4,pooling=True)\n","    self.fc = nn.Linear(16*16*base_channels*4,out_features)\n","  \n","  def forward(self,x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    return self.fc(out.view(x.shape[0],-1))\n","    \n","# Decoder definition with a fully-connected layer and 3 BN-ReLU-COnv blocks and \n","class Decoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Decoder, self).__init__()\n","    self.base_channels = base_channels\n","    self.fc = nn.Linear(out_features,16*16*base_channels*4)\n","    self.layer3 = BNReLUConv(base_channels*4,base_channels*2,pooling=True)\n","    self.layer2 = BNReLUConv(base_channels*2,base_channels,pooling=True)\n","    self.layer1 = BNReLUConv(base_channels,1,pooling=False)\n","  \n","  def forward(self,x):\n","    out = self.fc(x)\n","    out = out.view(x.shape[0],self.base_channels*4,16,16)\n","    out = self.layer3(out)\n","    out = self.layer2(out)\n","    out = self.layer1(out)\n","    return torch.sigmoid(out)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VAE(nn.Module):\n","  def __init__(self, out_features=64,base_channels=16):\n","    super(VAE, self).__init__()\n","    # Initialize the encoder and decoder using a dimensionality out_features for the vector z\n","    self.out_features = out_features\n","    self.encoder = Encoder(out_features*2,base_channels)\n","    self.decoder = Decoder(out_features,base_channels)\n","\n","  # function to obtain the mu and sigma of z for a samples x\n","  def encode(self,x):\n","    aux = self.encoder(x)\n","    # get z mean\n","    z_mean = aux[:,0:self.out_features]\n","    # get z variance\n","    z_log_var = aux[:,self.out_features::]\n","    return z_mean, z_log_var\n","\n","  # function to generate a random sample z given mu and sigma\n","  def sample_z(self,z_mean,z_log_var):\n","    z_std = z_log_var.mul(0.5).exp() \n","    samples_unit_normal = torch.randn_like(z_mean)\n","    samples_z = samples_unit_normal*z_std + z_mean\n","    return samples_z\n","  \n","  def forward(self,x):\n","    z_mean, z_log_var = self.encode(x)\n","    samples_z = self.sample_z(z_mean,z_log_var)\n","    x_rec = self.decoder(samples_z)\n","    return x_rec, z_mean, z_log_var\n","\n","# Print summary ofCmode\n","print('MNISTK VAE Definition')\n","vae = VAE(32)\n","print(vae)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Kullback-Leibler regularization computation\n","def kl_divergence(z_mean,z_log_var):\n","  kl_loss = 0.5 * torch.sum(  (torch.exp(z_log_var) + z_mean**2 - 1.0 - z_log_var),axis=1)\n","  return kl_loss.mean()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TPhaB5uN5nub"},"source":["## Sol. 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import imageio\n","\n","def generate_images(vae, n_samples=64, device='cpu'):\n","    ### Generate random samples\n","    vae.eval()\n","    # Random vectors z~N(0,I)\n","    z = torch.randn((n_samples,vae.out_features)).to(device)\n","\n","    # Genearte images with the decoder from the random vectors\n","    x_rec = vae.decoder(z)\n","    \n","\n","    # Show synthetic images\n","    plt.figure(figsize=(9,9))\n","    plt.title('Generated Images')\n","    image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","    plt.imshow(image_grid.permute(1,2,0).detach().numpy(),cmap='gray')\n","    plt.show()\n","\n","def interpolate_images(vae, kl_weight, n_samples=64, n_iterpolations=50, device='cpu'):\n","    ### Generate random samples\n","    ### Generate random samples\n","    vae.eval()\n","\n","    # Sample a set of pairs z_init and z_final\n","    z_init = torch.randn((n_samples,vae.out_features)).to(device)*2.0\n","    z_final = torch.randn((n_samples,vae.out_features)).to(device)*2.0\n","\n","    # Compute interpolations between z_init and z_final\n","    # and generate an image for each interpolation.\n","    interpolation_images = []\n","    for interp in range(0,n_iterpolations):\n","        interp_0_1 = float(interp) / (n_iterpolations-1)\n","        z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","        x_rec = vae.decoder(z)\n","        image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","        image_grid = image_grid.permute(1,2,0).detach().numpy()\n","        # save the generated images in a list\n","        interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","\n","    # Concatenate the inversion of the list to generate a \"loop\" animation\n","    interpolation_images += interpolation_images[::-1]\n","\n","    # Generate and visualize a give showing the interpolation results.\n","    imname = results_path+'/vae_interpolation_CKL_'+str(kl_weight)+'.gif'\n","    imageio.mimsave(imname, interpolation_images, duration=10)\n","\n","    with open(imname,'rb') as f:\n","        from IPython.display import Image, display\n","        display(Image(data=f.read(), format='png',width=512,height=512))\n","        \n","def alternate_rec_ref(rec_images, ref_images):\n","    interleaved_images = np.empty((2 * len(ref_images), 64, 64,3), dtype=np.float32)\n","\n","    for i in range(len(ref_images)):\n","        interleaved_images[2*i] = ref_images[i]\n","        interleaved_images[2*i+1] = rec_images[i]\n","\n","    # Create the grid of images assume interleaved_images are numpy arrays\n","    plt.figure(figsize=(9,9))\n","    plt.title('Reconstructed Images')\n","    image_grid = make_grid(torch.from_numpy(interleaved_images),nrow=8,padding=1)\n","    print(image_grid.shape)\n","    plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","\n","# Train function stopping at every 2 epochs for the requested visualizations\n","def train_VAE(vae,  train_loader, optimizer, kl_weight=0.001, num_epochs=10, model_name='vae_mnist.ckpt', device='cpu'):\n","    vae.to(device)\n","    vae.train() # Set the model in train mode\n","    total_step = len(train_loader)\n","    losses_list = []\n","    rec_loss_list = []\n","    kl_loss_list = []\n","    criterion = nn.MSELoss() # Use mean-squared error to compare the original and reconstructe images\n","    samples_images = np.random.choice(79, 16, replace=False)\n","    \n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        rec_loss_avg = 0\n","        kl_loss_avg = 0\n","        nBatches = 0\n","        \n","\n","        for i, (images) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            images = images.to(device)\n","            if epoch == 0:\n","                ref_images = images\n","\n","            # Forward pass (get encoder variables and reconstructed images)\n","            x_rec, z_mean, z_log_var = vae(images)\n","            \n","            reconstruction_loss = criterion(x_rec, images) # Reconstruction loss (x,x_rec)\n","            \n","            kl_loss = kl_divergence(z_mean, z_log_var) # Compute KL divergecnes KL( N(mu_x,sigma_x) || N(0,I))\n","            \n","            # Backward and optimize reconstruction loss and kl regularization\n","            optimizer.zero_grad()\n","            loss = reconstruction_loss + kl_loss*kl_weight # we use a weight to balance the importance of the KL loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            rec_loss_avg += reconstruction_loss.cpu().item()\n","            kl_loss_avg += kl_loss.cpu().item()\n","            loss_avg = rec_loss_avg + kl_loss_avg*kl_weight\n","\n","            nBatches+=1\n","            if (i+1) % 100 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","                       \n","        print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        losses_list.append(loss_avg / nBatches)\n","        rec_loss_list.append(rec_loss_avg / nBatches)\n","        kl_loss_list.append(kl_loss_avg / nBatches)\n","\n","        if epoch % 2 == 0:\n","              \n","            rec_images,_,_ = vae(ref_images)\n","            rec_images=rec_images.cpu()\n","            ref_images_disp=ref_images.cpu()\n","            #get 16 samples from the batch\n","            rec_images = rec_images[samples_images]\n","            ref_images_disp = ref_images_disp[samples_images]\n","\n","            # create a mixed grid with the original and reconstructed images\n","            interleaved_images =  torch.empty((2 * len(ref_images_disp), 3, 64, 64), dtype=torch.float32)\n","            interleaved_images[0::2] = ref_images_disp\n","            interleaved_images[1::2] = rec_images\n","\n","            fig=plt.figure(figsize=(11,6))\n","            plt.title('Reconstruction of the images for epoch '+str(epoch+1)+' with KL weight '+str(kl_weight),fontsize=16,fontweight='bold')\n","            plt.axis('off')\n","            fig.text(0.5,0.9,' [Rec Loss: '+str(np.round(rec_loss_avg/ nBatches,4))+' KL Loss: '+str(np.round(kl_loss_avg/ nBatches,4))+']', ha='center')\n","            image_grid = make_grid(interleaved_images,nrow=8,padding=1)\n","            plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","            \n","            #save the image\n","            imname = results_path+'/vae_reconstruction_CK_epoch_'+str(epoch+1)+ '_KL_weight_'+str(kl_weight)+'.png'\n","            plt.savefig(imname, bbox_inches='tight')\n","            plt.show()\n","            \n","            \n","            print(\"-----------------------Generated images:-----------------------\\n\")\n","            generate_images(vae, n_samples=16, device=device)\n","           \n","            print(\"-----------------------Interpolated images:-----------------------\\n\")\n","            interpolate_images(vae,kl_weight, n_samples=16, n_iterpolations=50, device=device)\n","\n","            # z = torch.randn_like(z_mean)\n","            # x_rec = vae.decoder(z)\n","            # plt.figure(figsize=(9,9))\n","            # plt.title('Generated Images')\n","            # image_grid = make_grid(x_rec.cpu(),nrow=4,padding=1)\n","            # plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","            # plt.show()\n","\n","    \n","    # save trained model\n","    torch.save(vae.state_dict(), results_path+ '/' + model_name)\n","          \n","    return losses_list, rec_loss_list, kl_loss_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Train a VAE on the CK dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num=1000.002344324\n","print('hola'+str(np.round(num,4)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#release GPU memory\n","torch.cuda.empty_cache()\n","\n","vae = VAE(32)\n","kl_weight=0.00005 \n","\n","#Initialize optimizer \n","learning_rate = .001\n","optimizer = torch.optim.Adam(vae.parameters(),lr = learning_rate, weight_decay=1e-5)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","loss_list,rec_loss_list, kl_loss_list = train_VAE(vae, train_loader, optimizer, kl_weight=kl_weight,\n","                      num_epochs=40, model_name='vae_ck_KL_weight_'+str(kl_weight)+'.cpkt', device=device)\n","\n","# Plot the losses\n","plt.figure(figsize=(10,5))\n","plt.plot(loss_list, label='Total loss')\n","plt.plot(rec_loss_list, label='Reconstruction loss')\n","plt.plot(kl_loss_list, label='KL loss')\n","plt.legend()\n","plt.title('Losses for VAE with KL weight '+str(kl_weight))\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","impath = results_path+'/vae_losses_KL_weight_'+str(kl_weight)+'.png'\n","plt.savefig(impath, bbox_inches='tight')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","vae_kl_0001 = VAE(32)\n","#import the trained model\n","vae_kl_0001.load_state_dict(torch.load(results_path+'/vae_ck_KL_weight_0.001.cpkt'))\n","vae_kl_0001.eval()\n","vae_kl_0001.to(device)\n","#generate images\n","generate_images(vae_kl_0001, n_samples=16, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display, clear_output\n","%matplotlib inline\n","def animation_reconstruciont(kl_weight,sleep, epochs):\n","    plt.figure(figsize=(9,9))\n","    for i in range(epochs//2):\n","        image = plt.imread(results_path+'/vae_reconstruction_CK_epoch_'+str(2*i+1)+ '_KL_weight_'+str(kl_weight)+'.png')\n","        plt.imshow(image)\n","        plt.xticks([])\n","        plt.yticks([])\n","        display(wait=True)\n","        plt.pause(sleep)\n","        plt.show()\n","        clear_output()\n","    plt.imshow(image)\n","    plt.show()\n","\n","def gif_reconstruction(kl_weight,epochs,duration):\n","    images = []\n","    for i in range(epochs//2):\n","        images.append(imageio.imread(results_path+'/vae_reconstruction_CK_epoch_'+str(2*i+1)+ '_KL_weight_'+str(kl_weight)+'.png'))\n","    \n","    path=results_path+'/vae_reconstruction_CK_KL_weight_'+str(kl_weight)+'.gif'\n","    imageio.mimsave(path, images, duration=duration)\n","    \n","    with open(path,'rb') as f:\n","        from IPython.display import Image, display\n","        display(Image(data=f.read(), format='png'))\n","\n","        \n","\n","    \n","animation_reconstruciont(kl_weight=kl_weight,sleep=0.15, epochs=40)\n","gif_reconstruction(kl_weight=kl_weight,epochs=40,duration=3)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FhQbpBkQ5ux0"},"source":["# Ex. 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1653800688561,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"J5zT1eRf5ux4","outputId":"9236d987-b00b-45e1-9669-07756d7a9ea5"},"outputs":[],"source":["'''\n","1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Generate and show a set of images from random noise z. \n","  2.2. Visualize a set of generated images by interpolating over the latent space z.\n","  2.3 Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\n","Compare the results with the ones obtained with VAEs\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vUrqAof5JRjH"},"source":["## Sol. 2"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"albertkernel","language":"python","name":"albertkernel"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
