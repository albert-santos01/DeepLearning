{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"IvcOz1eTrxRx"},"source":["# **Deep Generative Models: Variational Autoencoders and Generative Adversarial Networks**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EvWs2XIYNfhg"},"source":["# **CK Dataset**\n","In the following exercices, you will work with images extracted from the CK dataset: http://www.jeffcohn.net/wp-content/uploads/2020/02/Cohn-Kanade_Database.pdf.pdf\n","\n","It contains gray-scale images of human faces.\n","\n","The dataset is provided in the folder Data/faces/ in .mat format.\n","In the following we provide a Dataset class in pytorch to load images from this database."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":290},"executionInfo":{"elapsed":43049,"status":"ok","timestamp":1653800688560,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"5kE5D0hh1Ndz","outputId":"9483add6-4ad6-45c9-f5df-a2326394b6d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Path is not defined, initialized to c:/eduard/uni/3er/trim3/DeepLearning/DeepLearning/P4\n","torch.Size([256, 1, 64, 64])\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\erodriguez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:136: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["## Create a Custom Dataset for CK database\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import os\n","# Mount Google Drive\n","\n","try:\n","    print(\"Currently on path\", base_path)\n","except NameError:\n","    base_path = os.getcwd().replace('\\\\', '/')\n","    print(\"Path is not defined, initialized to\", base_path)\n","\n","data_path = base_path+'/Data/'\n","results_path = base_path+'/Results/'\n","\n","# All the data will be loaded from the provided file in Data/mnist.t\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as tf\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import scipy.io as sio\n","from torchvision.utils import make_grid\n","\n","\n","\n","#Making native class loader\n","class FacesDB(torch.utils.data.Dataset):\n","    # Initialization method for the dataset\n","    def __init__(self,dataDir = data_path+'/faces/face_ims_64x64.mat', transform = None):\n","        mat_loaded = sio.loadmat(dataDir)\n","        self.data = mat_loaded['X']\n","        self.transform = transform\n","\n","    # What to do to load a single item in the dataset ( read image and label)    \n","    def __getitem__(self, index):\n","        data = self.data[:,:,0,index]   \n","        data = Image.fromarray(data,mode='L')\n","        # Apply a trasnformaiton to the image if it is indicated in the initalizer\n","        if self.transform is not None : \n","            data = self.transform(data)\n","        \n","        # return the image and the label\n","        return data\n","\n","    # Return the number of images\n","    def __len__(self):\n","        return self.data.shape[3]\n","\n","import torchvision.transforms as transforms\n","\n","tr = transforms.Compose([\n","        transforms.ToTensor(), \n","        ])\n","faces_db = FacesDB(data_path+'/faces/face_ims_64x64.mat',tr)\n","train_loader = torch.utils.data.DataLoader(dataset=faces_db,\n","                                           batch_size=256, \n","                                           shuffle=True)\n","\n","# Mini-batch images\n","images = next(iter(train_loader))\n","print(images.shape)\n","image = images[0,:,:,:].repeat(3,1,1)\n","plt.imshow(image.permute(1,2,0).squeeze().numpy())\n","plt.axis('off')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CVA6m-IgNace"},"source":["# Ex. 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1653800688561,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"y1yTECVjDVmf","outputId":"5285d823-5808-45c8-bbd9-c18510b02767"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\\n2. For every two epochs during training:\\n  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\\n  2.2. Generate and show a set of images from random noise z. \\n  2.3. Visualize a set of generated images by interpolating over the latent space z.\\n  2.4 Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\\n'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","1. Following the example of the MNIST , train a VAE with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# Convolution + BatchNormnalization + ReLU block for the encoder\n","class ConvBNReLU(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(ConvBNReLU, self).__init__()\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","    self.bn = nn.BatchNorm2d(out_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.AvgPool2d(2,2)\n","\n","  def forward(self,x):\n","    if(self.pool):\n","      out = self.pool(x)\n","    else:\n","      out = x\n","    out = self.relu(self.bn(self.conv(out)))   \n","    return out\n","\n","#  BatchNormnalization + ReLU block + Convolution for the decoder\n","class BNReLUConv(nn.Module):\n","  def __init__(self,in_channels, out_channels, pooling=False):\n","    super(BNReLUConv, self).__init__()\n","    self.bn = nn.BatchNorm2d(in_channels)\n","    self.relu = nn.ReLU(inplace=True)\n","    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=3,\n","                          padding = 1)\n","\n","    self.pool = None\n","    if(pooling):\n","      self.pool = nn.UpsamplingNearest2d(scale_factor=2)\n","\n","  def forward(self,x):\n","    out = self.relu(self.bn(x))\n","    if(self.pool):\n","      out = self.pool(out)\n","    out = self.conv(out)\n","    return out\n","\n","# Encoder definition with 3 COnv-BN-ReLU blocks and fully-connected layer\n","class Encoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Encoder, self).__init__()\n","    self.layer1 = ConvBNReLU(1,base_channels,pooling=False)\n","    self.layer2 = ConvBNReLU(base_channels,base_channels*2,pooling=True)\n","    self.layer3 = ConvBNReLU(base_channels*2,base_channels*4,pooling=True)\n","    self.fc = nn.Linear(16*16*base_channels*4,out_features)\n","  \n","  def forward(self,x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = self.layer3(out)\n","    return self.fc(out.view(x.shape[0],-1))\n","    \n","# Decoder definition with a fully-connected layer and 3 BN-ReLU-COnv blocks and \n","class Decoder(nn.Module):\n","  def __init__(self,out_features,base_channels=16):\n","    super(Decoder, self).__init__()\n","    self.base_channels = base_channels\n","    self.fc = nn.Linear(out_features,16*16*base_channels*4)\n","    self.layer3 = BNReLUConv(base_channels*4,base_channels*2,pooling=True)\n","    self.layer2 = BNReLUConv(base_channels*2,base_channels,pooling=True)\n","    self.layer1 = BNReLUConv(base_channels,1,pooling=False)\n","  \n","  def forward(self,x):\n","    out = self.fc(x)\n","    out = out.view(x.shape[0],self.base_channels*4,16,16)\n","    out = self.layer3(out)\n","    out = self.layer2(out)\n","    out = self.layer1(out)\n","    return torch.sigmoid(out)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VAE(nn.Module):\n","  def __init__(self, out_features=64,base_channels=16):\n","    super(VAE, self).__init__()\n","    # Initialize the encoder and decoder using a dimensionality out_features for the vector z\n","    self.out_features = out_features\n","    self.encoder = Encoder(out_features*2,base_channels)\n","    self.decoder = Decoder(out_features,base_channels)\n","\n","  # function to obtain the mu and sigma of z for a samples x\n","  def encode(self,x):\n","    aux = self.encoder(x)\n","    # get z mean\n","    z_mean = aux[:,0:self.out_features]\n","    # get z variance\n","    z_log_var = aux[:,self.out_features::]\n","    return z_mean, z_log_var\n","\n","  # function to generate a random sample z given mu and sigma\n","  def sample_z(self,z_mean,z_log_var):\n","    z_std = z_log_var.mul(0.5).exp() \n","    samples_unit_normal = torch.randn_like(z_mean)\n","    samples_z = samples_unit_normal*z_std + z_mean\n","    return samples_z\n","  \n","  def forward(self,x):\n","    z_mean, z_log_var = self.encode(x)\n","    samples_z = self.sample_z(z_mean,z_log_var)\n","    x_rec = self.decoder(samples_z)\n","    return x_rec, z_mean, z_log_var\n","\n","# Print summary ofCmode\n","print('MNISTK VAE Definition')\n","vae = VAE(32)\n","print(vae)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["## Kullback-Leibler regularization computation\n","def kl_divergence(z_mean,z_log_var):\n","  kl_loss = 0.5 * torch.sum(  (torch.exp(z_log_var) + z_mean**2 - 1.0 - z_log_var),axis=1)\n","  return kl_loss.mean()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TPhaB5uN5nub"},"source":["## Sol. 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","For every two epochs during training:\n","  2.1. Visualize a set of reconstructed images and compute the reconstruction error over the whole dataset\n","  2.2. Generate and show a set of images from random noise z. \n","  2.3. Visualize a set of generated images by interpolating over the latent space z.\n","  2.4. Discuss the different visualizations by analysing their relation with the evolution of the reconstruction loss and the KL regularization term.\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import imageio\n","\n","def generate_images(vae, n_samples=64, device='cpu'):\n","    ### Generate random samples\n","    vae.eval()\n","    # Random vectors z~N(0,I)\n","    z = torch.randn((n_samples,vae.out_features)).to(device)\n","\n","    # Genearte images with the decoder from the random vectors\n","    x_rec = vae.decoder(z)\n","\n","    # Show synthetic images\n","    plt.figure(figsize=(9,9))\n","    plt.title('Generated Images')\n","    image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","    plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","\n","def interpolate_images(vae, n_samples=64, n_iterpolations=50, device='cpu'):\n","    ### Generate random samples\n","    ### Generate random samples\n","    vae.eval()\n","\n","    # Sample a set of pairs z_init and z_final\n","    z_init = torch.randn((n_samples,vae.out_features)).to(device)*2\n","    z_final = torch.randn((n_samples,vae.out_features)).to(device)*2\n","\n","    # Compute interpolations between z_init and z_final\n","    # and generate an image for each interpolation.\n","    interpolation_images = []\n","    for interp in range(0,n_iterpolations):\n","        interp_0_1 = float(interp) / (n_iterpolations-1)\n","        z = z_init*interp_0_1 + z_final*(1-interp_0_1)\n","        x_rec = vae.decoder(z)\n","        image_grid = make_grid(x_rec.cpu(),nrow=8,padding=1)\n","        image_grid = image_grid.permute(1,2,0).detach().numpy()\n","        # save the generated images in a list\n","        interpolation_images.append((image_grid*255.0).astype(np.uint8))\n","\n","    # Concatenate the inversion of the list to generate a \"loop\" animation\n","    interpolation_images += interpolation_images[::-1]\n","\n","    # Generate and visualize a give showing the interpolation results.\n","    imname = results_path+'/vae_interpolation_CK.gif'\n","    imageio.mimsave(imname, interpolation_images, fps=25)\n","\n","    with open(imname,'rb') as f:\n","        display(Image(data=f.read(), format='png',width=512,height=512))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","# Train function stopping at every 2 epochs for the requested visualizations\n","def train_VAE(vae,  train_loader, optimizer, kl_weight=0.001, num_epochs=10, model_name='vae_mnist.ckpt', device='cpu'):\n","    vae.to(device)\n","    vae.train() # Set the model in train mode\n","    total_step = len(train_loader)\n","    losses_list = []\n","    criterion = nn.MSELoss() # Use mean-squared error to compare the original and reconstructe images\n","    \n","    total_batch_size = len(train_loader)\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","        # Iterate the dataset\n","        rec_loss_avg = 0\n","        kl_loss_avg = 0\n","        nBatches = 0\n","        keep_samples = np.random.choice(total_batch_size, 16, replace=False)\n","        ref_images = np.array([])\n","        rec_images = np.array([])\n","        for i, (images, labels) in enumerate(train_loader):\n","            # Get batch of samples and labels\n","            images = images.to(device)\n","\n","            # Forward pass (get encoder variables and reconstructed images)\n","            x_rec, z_mean, z_log_var = vae(images)\n","            if i in keep_samples:\n","                idx = random.randint(0,images.shape[0]-1)\n","                rec_images = np.append(rec_images, x_rec[idx,:,:,:].repeat(3,1,1).permute(1,2,0).squeeze().numpy())\n","                ref_images = np.append(ref_images, images[idx,:,:,:].repeat(3,1,1).permute(1,2,0).squeeze().numpy())\n","            print(z_mean.shape)\n","            reconstruction_loss = criterion(x_rec, images) # Reconstruction loss (x,x_rec)\n","            \n","            kl_loss = kl_divergence(z_mean, z_log_var) # Compute KL divergecnes KL( N(mu_x,sigma_x) || N(0,I))\n","            \n","            # Backward and optimize reconstruction loss and kl regularization\n","            optimizer.zero_grad()\n","            loss = reconstruction_loss + kl_loss*kl_weight # we use a weight to balance the importance of the KL loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            rec_loss_avg += reconstruction_loss.cpu().item()\n","            kl_loss_avg += kl_loss.cpu().item()\n","\n","            nBatches+=1\n","            if (i+1) % 100 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Rec. Loss: {:.4f}, KL Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, rec_loss_avg / nBatches, kl_loss_avg / nBatches))\n","        losses_list.append(rec_loss_avg / nBatches)\n","\n","        if epoch % 2 == 0:\n","            print(\"-----------------------Reconstruced images:\\n-----------------------\")\n","            # create a grid of images of 4 rows and 8 columns (32 images in total) where you use the images in ref_images and rec_images \n","            # intercalated (ref,rec,ref,rec,ref,rec,ref,rec,ref,rec,ref,rec,ref,rec,ref,rec)\n","            plt.figure(figsize=(9,9))\n","            plt.title('Reconstructed Images')\n","            image_grid = make_grid(torch.from_numpy(np.concatenate((ref_images,rec_images))).view(32,3,64,64),nrow=8,padding=1)\n","            plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","            plt.show()\n","            print(\"For which the corresponding MSE is: \")\n","            print(criterion(torch.from_numpy(np.concatenate((ref_images,rec_images))).view(32,3,64,64),torch.from_numpy(np.concatenate((ref_images,rec_images))).view(32,3,64,64)).item())\n","            \n","\n","            print(\"-----------------------Generated images:\\n-----------------------\")\n","            generate_images(vae, n_samples=64, device=device)\n","            print(\"-----------------------Interpolated images:\\n-----------------------\")\n","            interpolate_images(vae, n_samples=64, n_iterpolations=50, device=device)\n","\n","            # z = torch.randn_like(z_mean)\n","            # x_rec = vae.decoder(z)\n","            # plt.figure(figsize=(9,9))\n","            # plt.title('Generated Images')\n","            # image_grid = make_grid(x_rec.cpu(),nrow=4,padding=1)\n","            # plt.imshow(image_grid.permute(1,2,0).detach().numpy())\n","            # plt.show()\n","\n","    \n","    # save trained model\n","    torch.save(vae.state_dict(), results_path+ '/' + model_name)\n","          \n","    return losses_list "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FhQbpBkQ5ux0"},"source":["# Ex. 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1653800688561,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"J5zT1eRf5ux4","outputId":"9236d987-b00b-45e1-9669-07756d7a9ea5"},"outputs":[],"source":["'''\n","1. Following the example of the MNIST , train a GAN with the images we have provided for the CK dataset.\n","2. For every two epochs during training:\n","  2.1. Generate and show a set of images from random noise z. \n","  2.2. Visualize a set of generated images by interpolating over the latent space z.\n","  2.3 Discuss the different visualizations by analysing their relation between their quality and the evolution of the discriminator and generator losses.\n","Compare the results with the ones obtained with VAEs\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vUrqAof5JRjH"},"source":["## Sol. 2"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
