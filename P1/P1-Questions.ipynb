{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P1-Questions.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fXH0lVHuisCw"},"source":["# Ex. 1"]},{"cell_type":"code","metadata":{"id":"hYqQjgWlivdx"},"source":["'''\n","Steps : \n","  1. Load train.csv and test.csv containing a 2D dataset (features X and Y) with two different classes\n","  2. Visualize the dataset and normalize the dimensions of the samples to have zero mean and unit standard deviation\n","  3. Implement an MLP using Numpy in order to solve a classification problem trying to estimate the classes of the samples\n","  4. Train the MLP and visualize the decision boundary in 2D of the classification. Plot the training loss for each iteration.\n","  5. Use the learned MLP to estimate the classes of the data in test.csv. Compute the accuracy in\n","     training and testing dataset.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3Chuxpjiv3W"},"source":["## Sol. 1"]},{"cell_type":"code","metadata":{"id":"XhQU8t66wYJ9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZB8-00E6-NF"},"source":["# Ex. 2"]},{"cell_type":"code","metadata":{"id":"KcXbgAb47FI8"},"source":["'''\n","Instead of using the implemented optimizer with standard Gradient Descent, in this exercice you will use SGD with momentum. \n","This optimizer tipically accelerates the convergence of the optimization and, thus, it reduce the number of iterations during gradient-descent.\n","Steps : \n","  1. Try to understand how momentum is implemented reading carefully https://ruder.io/optimizing-gradient-descent/index.html#momentum\n","  2. Modifying OptimSGD in the examples, implement SGD with momentum in a new class call OptimMom. \n","  3. Train the MLP implmented in numpy with this new optimizer. \n","  4. Use different values for the momentum paramter [0,1] and plot the evolution of the training loss during gradient-descent. Compare the results using the different values and standard gradient descent.\n","'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9aMFlbO7Fj4"},"source":["## Sol. 2"]},{"cell_type":"code","metadata":{"id":"FKMkugDvwXgQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CiIne9w6PkRr"},"source":["# Ex. 3"]},{"cell_type":"code","metadata":{"id":"V03WbzcvRzDl"},"source":["'''\n","Steps : \n","  1. Do the same as Ex. 1 but implementing the MLP using PyTorch.\n","  2. Train different vesions of the MLP by varying the number of hidden neurons and learing rates\n","  3. Analyse the results in each case and discuss them.\n","\n","HINT: Check in https://pytorch.org/docs/stable/nn.html what loss function implemented in PyTorch you have to use in this case.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qr0ElP9vRzvQ"},"source":["## Sol. 3"]},{"cell_type":"code","metadata":{"id":"yzUBzYUvwbW6"},"source":[""],"execution_count":null,"outputs":[]}]}